[{"title":"使用vscode和qemu调试linux内核（包含汇编）","date":"2022-04-26T03:42:50.000Z","path":"2022/04/26/使用vscode和qemu调试linux内核（包含汇编）/","text":"vscode调试kernel的汇编部分有两个麻烦点 linux加载地址和链接地址不同 vscode的默认setupCommands无法正常使用 通过一段时间的摸索我找到了调试这部分代码的方法 对于加载地址问题，可以通过gdb的add-symbol-file的-o参数来解决（这要求gdb的版本较新才可以） 对于vscode的配置，后面我会列出目前摸索出的配置文件供参考 调试步骤 编译linux内核，目前我使用的是linux-5.17.4内核 为了能更好的支持c的调试，我这边尽可能的将C代码通过O0编译，对kernel做了如下修改： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130diff --git a/arch/arm64/Makefile b/arch/arm64/Makefileindex 2f1de8865..27ecec930 100644--- a/arch/arm64/Makefile+++ b/arch/arm64/Makefile@@ -213,3 +213,5 @@ define archhelp echo &apos; (distribution) /sbin/installkernel or&apos; echo &apos; install to $$(INSTALL_PATH) and run lilo&apos; endef++subdir-ccflags-y = -O0diff --git a/drivers/Makefile b/drivers/Makefileindex a110338c8..da99b36a1 100644--- a/drivers/Makefile+++ b/drivers/Makefile@@ -187,3 +187,5 @@ obj-$(CONFIG_GNSS) += gnss/ obj-$(CONFIG_INTERCONNECT) += interconnect/ obj-$(CONFIG_COUNTER) += counter/ obj-$(CONFIG_MOST) += most/++subdir-ccflags-y = -O0diff --git a/drivers/char/tpm/Makefile b/drivers/char/tpm/Makefileindex 66d39ea6b..252f4d9d2 100644--- a/drivers/char/tpm/Makefile+++ b/drivers/char/tpm/Makefile@@ -41,3 +41,5 @@ obj-$(CONFIG_TCG_XEN) += xen-tpmfront.o obj-$(CONFIG_TCG_CRB) += tpm_crb.o obj-$(CONFIG_TCG_VTPM_PROXY) += tpm_vtpm_proxy.o obj-$(CONFIG_TCG_FTPM_TEE) += tpm_ftpm_tee.o++subdir-ccflags-y = -O1diff --git a/drivers/net/phy/Makefile b/drivers/net/phy/Makefileindex b2728d00f..f369abb70 100644--- a/drivers/net/phy/Makefile+++ b/drivers/net/phy/Makefile@@ -85,3 +85,5 @@ obj-$(CONFIG_STE10XP) += ste10Xp.o obj-$(CONFIG_TERANETICS_PHY) += teranetics.o obj-$(CONFIG_VITESSE_PHY) += vitesse.o obj-$(CONFIG_XILINX_GMII2RGMII) += xilinx_gmii2rgmii.o++CFLAGS_phylink.o = -O1diff --git a/include/net/sch_generic.h b/include/net/sch_generic.hindex 472843eed..06d6edcdf 100644--- a/include/net/sch_generic.h+++ b/include/net/sch_generic.h@@ -512,7 +512,7 @@ static inline bool lockdep_tcf_proto_is_locked(struct tcf_proto *tp) static inline void qdisc_cb_private_validate(const struct sk_buff *skb, int sz) &#123;- struct qdisc_skb_cb *qcb;+ struct qdisc_skb_cb *qcb __attribute__((unused)); BUILD_BUG_ON(sizeof(skb-&gt;cb) &lt; sizeof(*qcb)); BUILD_BUG_ON(sizeof(qcb-&gt;data) &lt; sz);diff --git a/init/Makefile b/init/Makefileindex 06326e304..9a0b7f269 100644--- a/init/Makefile+++ b/init/Makefile@@ -35,3 +35,5 @@ quiet_cmd_compile.h = CHK $@ include/generated/compile.h: FORCE $(call cmd,compile.h)++subdir-ccflags-y = -O0diff --git a/kernel/Makefile b/kernel/Makefileindex a18d16973..aeaa9d6d8 100644--- a/kernel/Makefile+++ b/kernel/Makefile@@ -160,3 +160,5 @@ $(obj)/kheaders_data.tar.xz: FORCE $(call cmd,genikh) clean-files := kheaders_data.tar.xz kheaders.md5++subdir-ccflags-y = -O0diff --git a/kernel/bpf/Makefile b/kernel/bpf/Makefileindex c1a9be6a4..d4713f4e3 100644--- a/kernel/bpf/Makefile+++ b/kernel/bpf/Makefile@@ -40,3 +40,5 @@ obj-$(CONFIG_BPF_PRELOAD) += preload/ obj-$(CONFIG_BPF_SYSCALL) += relo_core.o $(obj)/relo_core.o: $(srctree)/tools/lib/bpf/relo_core.c FORCE $(call if_changed_rule,cc_o_c)++CFLAGS_core.o = -O1diff --git a/kernel/dma/Makefile b/kernel/dma/Makefileindex 0dd65ec1d..a09453711 100644--- a/kernel/dma/Makefile+++ b/kernel/dma/Makefile@@ -10,3 +10,5 @@ obj-$(CONFIG_SWIOTLB) += swiotlb.o obj-$(CONFIG_DMA_COHERENT_POOL) += pool.o obj-$(CONFIG_DMA_REMAP) += remap.o obj-$(CONFIG_DMA_MAP_BENCHMARK) += map_benchmark.o++CFLAGS_direct.o = -O1diff --git a/lib/Makefile b/lib/Makefileindex 300f569c6..98aa9a698 100644--- a/lib/Makefile+++ b/lib/Makefile@@ -398,3 +398,5 @@ $(obj)/$(TEST_FORTIFY_LOG): $(addprefix $(obj)/, $(TEST_FORTIFY_LOGS)) FORCE ifeq ($(CONFIG_FORTIFY_SOURCE),y) $(obj)/string.o: $(obj)/$(TEST_FORTIFY_LOG) endif++subdir-ccflags-y = -O0diff --git a/lib/zstd/Makefile b/lib/zstd/Makefileindex fc45339fc..96cb0ce27 100644--- a/lib/zstd/Makefile+++ b/lib/zstd/Makefile@@ -42,3 +42,5 @@ zstd_decompress-y := \\ decompress/zstd_ddict.o \\ decompress/zstd_decompress.o \\ decompress/zstd_decompress_block.o \\++subdir-ccflags-y = -O1diff --git a/mm/Makefile b/mm/Makefileindex 70d4309c9..c225b254c 100644--- a/mm/Makefile+++ b/mm/Makefile@@ -132,3 +132,12 @@ obj-$(CONFIG_PAGE_REPORTING) += page_reporting.o obj-$(CONFIG_IO_MAPPING) += io-mapping.o obj-$(CONFIG_HAVE_BOOTMEM_INFO_NODE) += bootmem_info.o obj-$(CONFIG_GENERIC_IOREMAP) += ioremap.o++subdir-ccflags-y = -O0+CFLAGS_truncate.o = -O1+CFLAGS_gup.o = -O1+CFLAGS_memory.o = -O1+CFLAGS_pagewalk.o = -O1+CFLAGS_page_io.o = -O1+CFLAGS_vmalloc.o = -O1+CFLAGS_swapfile.o = -O1 将vscode的launch按如下方式进行配置 123456789101112131415161718192021222324252627282930313233&#123; \"version\": \"0.2.0\", \"configurations\": [ &#123; \"name\": \"linux\", \"type\": \"cppdbg\", \"request\": \"launch\", \"program\": \"$&#123;workspaceFolder&#125;/vmlinux\", \"args\": [], \"stopAtEntry\": false, \"cwd\": \"$&#123;workspaceFolder&#125;\", \"environment\": [], \"externalConsole\": false, \"MIMode\": \"gdb\", \"miDebuggerPath\": \"/home/gngshn/.local/opt/aarch64-buildroot-linux-gnu/bin/aarch64-linux-gdb\", \"targetArchitecture\": \"arm64\", \"customLaunchSetupCommands\": [ &#123; \"text\": \"add-symbol-file vmlinux\" &#125;, &#123; \"text\": \"add-symbol-file vmlinux -o -0xffff7fffc7e00000\" &#125;, &#123; \"text\": \"b _text\" &#125;, &#123; \"text\": \"target remote :1234\" &#125;, ], &#125; ]&#125; 使用qemu打开linux 1qemu-system-aarch64 -M virt -cpu cortex-a53 -smp 4 -m 4G -nographic -append &quot;console=ttyAMA0&quot; -kernel arch/arm64/boot/Image -initrd ../rootfs.cpio.uboot -S -s 直接再vscode中按F5开始调试即可","tags":[{"name":"linux","slug":"linux","permalink":"http://gngshn.github.io/tags/linux/"},{"name":"vscode","slug":"vscode","permalink":"http://gngshn.github.io/tags/vscode/"},{"name":"gdb","slug":"gdb","permalink":"http://gngshn.github.io/tags/gdb/"}]},{"title":"利用numpy的矩阵运算实现自定义转换矩阵的YCbCr2RGB工具","date":"2018-01-19T03:04:12.000Z","path":"2018/01/19/利用numpy的矩阵运算实现自定义转换矩阵的YCbCr2RGB工具/","text":"OpenCV自带的cvtColor色彩空间转换矩阵的转换矩阵好像不太好替换, 目前我没有找到合适的方法来搞定. 最后自己研究了一下, 利用numpy的矩阵运算可以和cvtColor一样快的把YCbCr图片转换成RGB图片. 这里使用了numpy的多维array的dot乘法. 在numpy的document中, 写明了多维array的计算方式: dot(a, b)[i,j,k,m] = sum(a[i,j,:] * b[k,:,m]) 首先, 我们需要把NV12/NV16的YCbCr数据转换成YCbCr444模式. 之后, 目标是把YCbCr的三个通道换算成RGB通道, 也就是说通过一个3x3的矩阵左乘一个[Y, Cb, Cr].T, 就变成了[R, G, B].T了. 由于我们使用的YCbCr是基于BT709的FULL SWING的版本, 也就是说Y, Cb, Cr都是0-255范围的. 因此为了计算的正确性, 先把0~255的整数除以256换成浮点, 然后把Cb, Cr都减去0.5变成-0.5~0.5的范围. 然后做矩阵运算, 就可以算出0~1范围的RGB值了. 考虑YCbCr三个通道是[height x width x 3], 为了能让YCbCr能被[3 x 3]的矩阵左乘, 需要换一下通道的顺序, 变成[height x 3 x height], 通过numpy.swapaxis可以实现, 根据前面的dot乘法公式可以知道 [3 x 3] dot [height x 3 x width] --&gt; [3 x height x width] 因此算出的RGB通道同样需要切换一下顺序. 最后把计算好的RGB进行一下范围的限制, 防止溢出就可以了, 最终的程序如下 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970#!/usr/bin/env python3import fireimport matplotlib.pyplot as pltimport numpy as npimport osimport scipy.miscclass ShowYuv: _bt709_mat = np.linalg.inv(np.array([[0.213, 0.715, 0.072], [-0.115, -0.385, 0.500], [0.500, -0.454, -0.046]])) @staticmethod def _show_rgb(rgb): plt.imshow(rgb) plt.show() def _save_rgb(rgb, filename): if filename is not None: print(f'saving &#123;filename&#125;...') scipy.misc.imsave(filename, rgb) def _process_yuv444(yuv444, filename, bmp): rgb = ShowYuv._yuv2rgb(yuv444) ShowYuv._show_rgb(rgb) if bmp: ShowYuv._save_rgb(rgb, os.path.splitext(filename)[0] + '.bmp') @staticmethod def _yuv2rgb(yuv444): yuv444 = yuv444 / 256 yuv444[:, :, 1:] -= 0.5 yuv444 = np.transpose(yuv444, (0, 2, 1)) rgb = np.dot(ShowYuv._bt709_mat, yuv444) rgb = np.transpose(rgb, (1, 2, 0)) rgb = np.around(rgb * 256) rgb = np.clip(rgb, 0, 255).astype(np.uint8) return rgb @staticmethod def yuv422sp(filename, width, height, bmp=0): yuv422 = np.fromfile(filename, dtype=np.uint8) if 2 * width * height != yuv422.shape[0]: raise ValueError('width or height error') yuv444 = np.empty([height, width, 3], dtype=np.uint8) yuv444[:, :, 0] = yuv422[:width * height].reshape(height, width) u = yuv422[width * height::2].reshape(height, width // 2) yuv444[:, :, 1] = scipy.misc.imresize(u, (height, width)) v = yuv422[width * height + 1::2].reshape(height, width // 2) yuv444[:, :, 2] = scipy.misc.imresize(v, (height, width)) ShowYuv._process_yuv444(yuv444, filename, bmp) @staticmethod def yuv420sp(filename, width, height, bmp=0): yuv420 = np.fromfile(filename, dtype=np.uint8) if 3 * width * height // 2 != yuv420.shape[0]: raise ValueError('width or height error') yuv444 = np.empty([height, width, 3], dtype=np.uint8) yuv444[:, :, 0] = yuv420[:width * height].reshape(height, width) u = yuv420[width * height::2].reshape(height // 2, width // 2) yuv444[:, :, 1] = scipy.misc.imresize(u, (height, width)) v = yuv420[width * height + 1::2].reshape(height // 2, width // 2) yuv444[:, :, 2] = scipy.misc.imresize(v, (height, width)) ShowYuv._process_yuv444(yuv444, filename, bmp)if __name__ == '__main__': fire.Fire(ShowYuv)","tags":[{"name":"python","slug":"python","permalink":"http://gngshn.github.io/tags/python/"},{"name":"numpy","slug":"numpy","permalink":"http://gngshn.github.io/tags/numpy/"}]},{"title":"arm64_linux启动流程分析08_正式跳入内核空间虚拟地址段运行","date":"2017-11-30T04:00:54.000Z","path":"2017/11/30/arm64-linux启动流程分析08-正式跳入内核空间虚拟地址段运行/","text":"相信一些细心的读者有注意到, 到目前位置, 内核仍然在低地址段运行, 虽然我们已经启动了MMU, 但是仍然运行在等于物理地址的虚拟地址上, 下面我们就要正式的切换到内核态的高地址空间来运行. 123ldr x8, =__primary_switchedadrp x0, __PHYS_OFFSETblr x8 这段code是上篇遗留的一点内容, 现在来进行分析. 第一条指令是arm的伪指令, 将__primary_switched标签的链接地址放在x8中, 第二条将__PHYS_OFFSET对应的运行时地址存在在x0中, __PHYS_OFFSET虽然名字有PHYS, 但是实际他是指_text - TEXT_OFFSET的链接地址. 它等于0xffff_0000_0000_0000 + 128M(module) + kaslr, 那现在x0存放的应该就是 DDR起始地址 + 2M align预留内存 + kaslr, 然后跳转到x8执行 这里有一点需要注意, 第一条指令加载的是链接地址, 我们现在放置的位置和链接地址是有kaslr的偏移的, 他是如何跳对的呢? 第一条指令实际上变汇编成一条指令加一个内存池, 如: 123 ldr x8 __priary_switched_addr__priary_switched_addr: 0xffff_xxxx_xxxx_xxxx(也就是__primary_switched的链接地址) 这个内存池实际上就会导致上一篇内容讲的.rela段增加3条内容. 因此在上篇的relocate运行时, 这个地址就已经被修正了. 自然就能跳转到正确的位置了. 来看看__primary_switched的内容(特别注意, 从跳转发生开始, 内核就开始运行到高的虚拟地址上了): 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990__primary_switched: /* * 把current SP指到`init_thread_union`的最高位置(ARM64和thread_info和 * stack不共用, thread_info放在task_strcut里面, init_thread_union只是用 * 于内核堆栈) */ adrp x4, init_thread_union add sp, x4, #THREAD_SIZE /* * 把EL0的sp放在init_task下面, 原因不明, 暂时放着, 目前看起来kernel好像 * 有把sp_el0设定在task_struct下面, 这样current实现起来会很容易直接把 * sp_el0类型转换一下就能返回进程的task_struct, 另外把thread_info也放在 * task_struct的第一个元素, 这个current_thread_info()也就是将current直接 * 强制类型转换一下就OK. */ adr_l x5, init_task msr sp_el0, x5 // Save thread_info /* 初始化内核的异常向量表, 关于向量表的泪容后续还会在分析 */ adr_l x8, vectors // load VBAR_EL1 with virtual msr vbar_el1, x8 // vector table address isb /* * 这一段code就是满足AARCH64 PCS(AArch64 Procedure Call Standard)来写的, * PCS规定x29是FP寄存器, x30是lr寄存器, 在函数被调用时, 首先将fp和lr压 * 栈, 然后将sp保存在fp寄存器中. 这里就是完成了这样一个过程, 从而如果调 * 用了C code(kaslr就可能会调用设备树相关的C code), C code返回时, 能正确 * 的恢复sp, 从而我们也还能正常的返回到前面的`__primary_switch`函数中去, * 由于这里fp没有的, 因此用xzr代替了 */ stp xzr, x30, [sp, #-16]! mov x29, sp /* 把设备树的物理地址存到`__fdt_pointer`中. */ str_l x21, __fdt_pointer, x5 // Save FDT pointer /* * kimage_vaddr就是`.quad _text - TEXT_OFFSET`, 这个值已经 * 被relocate修正了kaslr, 所以就是内核目前起始虚拟地址-TEXT_OFFSET, * 也就是0xffff_0000_0000_0000 + 128M(module) + kaslr, * 减掉x0之后就是内核虚拟地址和当前所在的物理地址的offset, 将其存放在 * kimage_voffset中. */ ldr_l x4, kimage_vaddr // Save the offset between sub x4, x4, x0 // the kernel virtual and str_l x4, kimage_voffset, x5 // physical mappings // Clear BSS adr_l x0, __bss_start mov x1, xzr adr_l x2, __bss_stop sub x2, x2, x0 /* 这里的`__pi_memset`定义在哪里还没搞清楚 */ bl __pi_memset dsb ishst // Make zero page visible to PTW#ifdef CONFIG_KASAN /* KASAN是一个中调试技术, 用来标记内存, 没细致研究 */ bl kasan_early_init#endif#ifdef CONFIG_RANDOMIZE_BASE /* * 在第4篇文章中, 我们说到x23保存的是KASLR区域的大小. 如果这里判断如果 * x23为0, 那么表示内核没有被放在一个随机地址, 也就是前面说的第二种kaslr * 启动方式没有被 使用, 那么将调用C code `kaslr_early_init`来获取 * `kaslr-seed`并搬运内核之后返回`__primary_switch`继续执行 关闭MMU * relocate 开启MMU, 再调过来的过程, 就不细说了 */ tst x23, ~(MIN_KIMG_ALIGN - 1) // already running randomized? b.ne 0f mov x0, x21 // pass FDT address in x0 bl kaslr_early_init // parse FDT for KASLR options cbz x0, 0f // KASLR disabled? just proceed orr x23, x23, x0 // record KASLR offset ldp x29, x30, [sp], #16 // we must enable KASLR, return ret // to __primary_switch()0:#endif /* * 这里对sp出栈(增加sp)的上面的出栈只有一个会运行, 要么出栈返回, 要么在 * 这里假装出栈, 最后调用`start_kernel`, 由于此处`start_kernel`不会返回, * 我们也无须返回, 所以可以把lr和fp的内容直接抹掉了. */ add sp, sp, #16 mov x29, #0 mov x30, #0 /* 欢快的进入`start_kernel` C code取执行KERNEL初始化了. */ b start_kernelENDPROC(__primary_switched) 以上就是arm64 linux启动过程的汇编部分的分析.","tags":[{"name":"linux","slug":"linux","permalink":"http://gngshn.github.io/tags/linux/"},{"name":"arm64","slug":"arm64","permalink":"http://gngshn.github.io/tags/arm64/"}]},{"title":"arm64_linux启动流程分析07_开启MMU切换到虚拟地址","date":"2017-11-30T04:00:44.000Z","path":"2017/11/30/arm64-linux启动流程分析07-开启MMU切换到虚拟地址/","text":"这篇内容我们来讲述__primary_switch 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657__primary_switch:#ifdef CONFIG_RANDOMIZE_BASE /* * 保存x0(开启MMU时需要写入sctlr_el1的值, 保存sctlr_el1的当前值 * 这两条汇编的作用是为了在开启MMU, 内核发现运行地址和链接地址是一致的时候再次关闭 * MMU, 然后通过获取设备树的`kaslr-seed`来将内核移动到一个随机地址运行, 当然如果 * 设备树没有`kaslr-seed`或者command line指定了`nokaslr`则就不启动kaslr(也就 * 不用返回重启MMU, 重新建立页表了 */ mov x19, x0 // preserve new SCTLR_EL1 value mrs x20, sctlr_el1 // preserve old SCTLR_EL1 value#endif /* 开启MMU */ bl __enable_mmu#ifdef CONFIG_RELOCATABLE /* 如果因为kaslr的关系, 目前内核的运行地址和链接地址是不同的, 前面的code能正常运行 * 是因为所有的指令和数据都是PIC(代码位置无关)的, 后面的code是不能保证的, 所以需要 * 用内核的`.rela`段来修复这个问题 */ bl __relocate_kernel#ifdef CONFIG_RANDOMIZE_BASE /* 在这里准备swapper的初始化环境, 后面正式跳入C code进行内核初始化, 这个下篇再说 */ ldr x8, =__primary_switched adrp x0, __PHYS_OFFSET blr x8 /* * If we return here, we have a KASLR displacement in x23 which we need * to take into account by discarding the current kernel mapping and * creating a new one. */ /* * 如果返回到这里表示使用设备树的`kaslr-seed`来启动kaslr, 这里内核已经被重新copy * 到(0xffff_0000_0000_0000 + 128M(module) + kaslr-seed + TEXT_OFFSET的 * 位置, 因此内核的页表, 符号都需要重新进行设定. 所以下面的code关闭MMU, 重建页表, * 修复内核符号, 再次跳入__primary_switched */ msr sctlr_el1, x20 // disable the MMU isb bl __create_page_tables // recreate kernel mapping tlbi vmalle1 // Remove any stale TLB entries dsb nsh msr sctlr_el1, x19 // re-enable the MMU isb ic iallu // flush instructions fetched dsb nsh // via old mapping isb bl __relocate_kernel#endif#endif ldr x8, =__primary_switched adrp x0, __PHYS_OFFSET br x8ENDPROC(__primary_switch) 通过上面的注释, 我们可以大致了解到code的行为和, 我们我们来关注一下行为的细节. 首先是__enable_mmu 12345678910111213141516171819202122232425262728293031323334353637383940414243ENTRY(__enable_mmu) /* * 下面4条代码判断硬件是否实际支持4K页表映射, 如果不支持, 就跳入 * `__no_granule_support`, 表明启动失败, CPU调用wfe, wfi进入idle状态 */ mrs x1, ID_AA64MMFR0_EL1 ubfx x2, x1, #ID_AA64MMFR0_TGRAN_SHIFT, 4 cmp x2, #ID_AA64MMFR0_TGRAN_SUPPORTED b.ne __no_granule_support /* 在`__early_cpu_boot_status`存入0表明CPU状态正常 */ update_early_cpu_boot_status 0, x1, x2 /* * 设定TTBR0, TTBR1, TTBR0用于虚拟地址MSB都为0时(在这里对应与物理地址相 * 等的虚拟地址)的页表映射, TTBR1用于虚拟地址MSB都为1时(在这里对应与物理地 * 址相等的虚拟地址)的页表映射. 在正常状态下TTBR0用于用户空间页表映射, * TTBR1用于内核空间页表映射, 分成两个可以在切换进程时, 内核页表不用做切换 */ adrp x1, idmap_pg_dir adrp x2, swapper_pg_dir msr ttbr0_el1, x1 // load TTBR0 msr ttbr1_el1, x2 // load TTBR1 /* * 两条isb内存屏障保证中间的指令执行的顺序, isb还需flush cpu的pipe line, * 这样CPU就会在开启MMU之后重新取指令, 可以保证去到的指令的正确性. */ isb msr sctlr_el1, x0 isb /* * Invalidate the local I-cache so that any instructions fetched * speculatively from the PoC are discarded, since they may have * been dynamically patched at the PoU. */ /* * iallu = invalidate + all + PoU * 清空左右的icache到PoU保证cpu core的各个模块看到的内容的一致性, dsb nsh * 保证上面的动作在CORE本地(non-sharable)完成. 最后清空CPU pipeline, 返回 */ ic iallu dsb nsh isb retENDPROC(__enable_mmu) 然后是___relocate_kernel 123456789101112131415161718192021222324__relocate_kernel: /* * Iterate over each entry in the relocation table, and apply the * relocations in place. */ ldr w9, =__rela_offset // offset to reloc table ldr w10, =__rela_size // size of reloc table mov_q x11, KIMAGE_VADDR // default virtual offset add x11, x11, x23 // actual virtual offset add x9, x9, x11 // __va(.rela) add x10, x9, x10 // __va(.rela) + sizeof(.rela)0: cmp x9, x10 b.hs 1f ldp x11, x12, [x9], #24 ldr x13, [x9, #-8] cmp w12, #R_AARCH64_RELATIVE b.ne 0b add x13, x13, x23 // relocate str x13, [x11, x23] b 0b1: retENDPROC(__relocate_kernel) __rela_offset定义在链接文件中, 如下: 123456.rela : ALIGN(8) &#123; *(.rela .rela*) &#125; __rela_offset = ABSOLUTE(ADDR(.rela) - KIMAGE_VADDR); __rela_size = SIZEOF(.rela); 可以看出__rela_offset是.rela段相对KERNEL起始地址的偏移. __rela_size是.rela段的偏移.通过对汇编代码的阅读, 我们大致可以看出.rela段的内容如下: 1234| 64 bit | 64 bit | 64 bit | ...+----------------+-----------------------------------+-----------------| sym0 link addr | sym0 reloc flag | sym0 link value | sym1 link ...+----------------+-----------------------------------+----------------- 每个relocable symbol有三个entry, 每个entry都是32bit, 分别为link addr, relocation flag, link value. 对于需要重定位内核运行位置的情况, 我们需要把所有FLAG为R_AARCH64_RELATIVE的的entry指定的symbol进行修正, 修正方法为将link addr + offset的值修改为link value + offset. 其实这个段的含义就是说在IMAGE中, link addr的位置中存放的值是地址相关地值, 如果IMAGE发生了整体偏移, 那么link addr的位置实际在link addr + offset, 要把实际位置上的value修正, 这样说不知道是不是更清晰一些. 具体请参阅ELF for the ARM 64-bit Architecture (AArch64), 我也没有细致研究. __primary_switched相关的内容我们放到下一篇内容中, 每一篇内容都少点, 好理解一些.","tags":[{"name":"linux","slug":"linux","permalink":"http://gngshn.github.io/tags/linux/"},{"name":"arm64","slug":"arm64","permalink":"http://gngshn.github.io/tags/arm64/"}]},{"title":"arm64_linux启动流程分析06_CPU的一些初始化","date":"2017-11-30T04:00:36.000Z","path":"2017/11/30/arm64-linux启动流程分析06-CPU的一些初始化/","text":"接下来进入__cpu_setup函数 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677ENTRY(__cpu_setup) /* 清空当前CORE所有的tlb, vm + all + el1, 这条指令需要对non-sharable区域加内存屏障 */ tlbi vmalle1 // Invalidate local TLB dsb nsh /* 开启CPU的FP/SIMD, debug功能, 关闭PMU等 */ mov x0, #3 &lt;&lt; 20 msr cpacr_el1, x0 // Enable FP/ASIMD mov x0, #1 &lt;&lt; 12 // Reset mdscr_el1 and disable msr mdscr_el1, x0 // access to the DCC from EL0 isb // Unmask debug exceptions now, enable_dbg // since this is per-cpu reset_pmuserenr_el0 x0 // Disable PMU access from EL0 /* * 上面内容我们说到了, 页表entry表明内存是普通内存, 就是结合这里的初始化来指明的, * PMD_ATTRINDX(MT_NORMAL)是4, 这其实是一个index, 指向MAIR寄存器的[4*8+7:4*8], * MAIR寄存器一共有8组, KERNEL用了6组, 每组有8bit, 每个bit都有相应的含义. * 具体参考手册, 这里就不细说了, 点到为止 */ /* * Memory region attributes for LPAE: * * n = AttrIndx[2:0] * n MAIR * DEVICE_nGnRnE 000 00000000 * DEVICE_nGnRE 001 00000100 * DEVICE_GRE 010 00001100 * NORMAL_NC 011 01000100 * NORMAL 100 11111111 * NORMAL_WT 101 10111011 */ ldr x5, =MAIR(0x00, MT_DEVICE_nGnRnE) | \\ MAIR(0x04, MT_DEVICE_nGnRE) | \\ MAIR(0x0c, MT_DEVICE_GRE) | \\ MAIR(0x44, MT_NORMAL_NC) | \\ MAIR(0xff, MT_NORMAL) | \\ MAIR(0xbb, MT_NORMAL_WT) msr mair_el1, x5 /* * Prepare SCTLR */ /* 从这里开始注意x0寄存器, 保存了后面开启MMU时需要写入sctlr_el1的值 */ adr x5, crval ldp w5, w6, [x5] mrs x0, sctlr_el1 bic x0, x0, x5 // clear bits orr x0, x0, x6 // set bits /* * Set/prepare TCR and TTBR. We use 512GB (39-bit) address range for * both user and kernel. */ ldr x10, =TCR_TxSZ(VA_BITS) | TCR_CACHE_FLAGS | TCR_SMP_FLAGS | \\ TCR_TG_FLAGS | TCR_ASID16 | TCR_TBI0 tcr_set_idmap_t0sz x10, x9 /* * Read the PARange bits from ID_AA64MMFR0_EL1 and set the IPS bits in * TCR_EL1. */ mrs x9, ID_AA64MMFR0_EL1 bfi x10, x9, #32, #3#ifdef CONFIG_ARM64_HW_AFDBM /* * Hardware update of the Access and Dirty bits. */ mrs x9, ID_AA64MMFR1_EL1 and x9, x9, #0xf cbz x9, 2f cmp x9, #2 b.lt 1f orr x10, x10, #TCR_HD // hardware Dirty flag update1: orr x10, x10, #TCR_HA // hardware Access flag update2:#endif /* CONFIG_ARM64_HW_AFDBM */ msr tcr_el1, x10 ret // return to head.SENDPROC(__cpu_setup) 这段汇编代码初始化了CPU的一些寄存器, 如tcr_el1(translation control register)配置虚拟地址的大小, PAGE大小, ASID, share属性, TAG等, 并准备了开启MMU时需要写入sctlr_el1的值.","tags":[{"name":"linux","slug":"linux","permalink":"http://gngshn.github.io/tags/linux/"},{"name":"arm64","slug":"arm64","permalink":"http://gngshn.github.io/tags/arm64/"}]},{"title":"arm64_linux启动流程分析05_配置内核启动的临时页表","date":"2017-11-30T04:00:28.000Z","path":"2017/11/30/arm64-linux启动流程分析05-配置内核启动的临时页表/","text":"这篇内容讲讲页表的配置, 为了kernel运行速度加快, 我们需要启动cache, 启动cache需要先启动MMU让CPU运行到虚拟地址上, 那么我们就需要启动一个能覆盖KERNEL内存区域的页表. 本篇内容假设在您对MMU有一定的了解的基础上来讲述的. 我们假设使用4K页来管理内存, 同时虚拟地址使用48位地址 我们将__create_page_tables函数分成几段来讲解 12345678910111213141516171819202122__create_page_tables: mov x28, lr /* * Invalidate the idmap and swapper page tables to avoid potential * dirty cache lines being evicted. */ adrp x0, idmap_pg_dir ldr x1, =(IDMAP_DIR_SIZE + SWAPPER_DIR_SIZE + RESERVED_TTBR0_SIZE) bl __inval_dcache_area /* * Clear the idmap and swapper page tables. */ adrp x0, idmap_pg_dir ldr x1, =(IDMAP_DIR_SIZE + SWAPPER_DIR_SIZE + RESERVED_TTBR0_SIZE)1: stp xzr, xzr, [x0], #16 stp xzr, xzr, [x0], #16 stp xzr, xzr, [x0], #16 stp xzr, xzr, [x0], #16 subs x1, x1, #64 b.ne 1b 在vmlinux.lds.S中 12345. = ALIGN(PAGE_SIZE);idmap_pg_dir = .;. += IDMAP_DIR_SIZE;swapper_pg_dir = .;. += SWAPPER_DIR_SIZE; 所以idmap_pg_dir是在bss段后面且是PAGE_SIZE对齐的. x0保存idmap_pg_dir当前所在的物理地址. x1保存idmap_pg_dir和swapper_pg_dir的大小. 12#define SWAPPER_DIR_SIZE (SWAPPER_PGTABLE_LEVELS * PAGE_SIZE)#define IDMAP_DIR_SIZE (IDMAP_PGTABLE_LEVELS * PAGE_SIZE) 通过对宏的观察, 我们了解到idmap_pg_dir和swapper_pg_dir都是3个PAGE_SIZE(12K), 我们知道使用4K页加48bit虚拟地址需要4级页表才能满足, 每级页表都是一个PAGE level0 [47:39] 512个entry, 每个8byte, 一共4K level1 [38:30] 512个entry, 每个8byte, 一共4K level2 [29:21] 512个entry, 每个8byte, 一共4K level3 [20:13] 512个entry, 每个8byte, 一共4K 剩下的11:0地址用level3的内容拼接得到最终的物理地址. 而这里idmap_pg_dir和swapper_pg_dir都只有3个PAGE是如何来映射的呢? 其实这里KERNEL在早期的临时页表为了节省内存, 并没有使用标准的4级映射, 而是使用的MMU中的block来直接描述2M(使用低21bit)内存区, 而不是用entry来描述4K页, 从而节省了level3的页表. 也就是说上面的level2页表中, 每条entry指向的不是level3页表, 而是一个2M的内存区. 上面的代码把这里的6个page全部清零, 因此所有的entry都成了invalid. 12345678910111213141516/* * x7保存level3页表的entry的flags, 表明是普通内存, * 是一个block的entry, 后面细说 */mov x7, SWAPPER_MM_MMUFLAGS/* * Create the identity mapping. */adrp x0, idmap_pg_diradrp x3, __idmap_text_start // __pa(__idmap_text_start)create_pgd_entry x0, x3, x5, x6mov x5, x3 // __pa(__idmap_text_start)adr_l x6, __idmap_text_end // __pa(__idmap_text_end)create_block_map x0, x7, x3, x5, x6 x0保存idmap_pg_dir的物理地址. x3保存__idmap_text_start的物理地址. 1234. = ALIGN(SZ_4K); \\VMLINUX_SYMBOL(__idmap_text_start) = .; \\*(.idmap.text) \\VMLINUX_SYMBOL(__idmap_text_end) = .; 所以x3保存的是.idmap.text段所在的物理地址. 这个段是head.S文件的后半部分, 这部份code是CPU和MMU从关闭到开启的过程中执行的code, 说道这里你应该明白了, idmap_pg_dir对应的页表是用来将与KERNEL所在物理地址相等的虚拟地址映射到相同的物理地址. 从而保证开启MMU时, 不会发生无法获取页表的情况. 而swapper_pg_dir如其名是swapper进程运行所需的页表, 是内核初始化过程所用的页表. 另外ARM64有TTBR0, TTBR1(Translation Table Base Register)分别用来指示内核空间和用户空间页表所在的物理地址, 而在这个时候, TTBR0不是用来指示用户空间地址, 而是用来指示与物理地址相等的虚拟地址所用的页表. 所以TTBR0里面是.idmap.text的物理地址, TTBR1里面是swapper_pg_dir的物理地址. 123456789101112 .macro create_pgd_entry, tbl, virt, tmp1, tmp2 /* 这里PGDIR_SHIFT是39, PTRS_PER_PGD是512 */ create_table_entry \\tbl, \\virt, PGDIR_SHIFT, PTRS_PER_PGD, \\tmp1, \\tmp2 /* 使用4K页时SWAPPER_PGTABLE_LEVELS为3 */#if SWAPPER_PGTABLE_LEVELS &gt; 3 create_table_entry \\tbl, \\virt, PUD_SHIFT, PTRS_PER_PUD, \\tmp1, \\tmp2#endif#if SWAPPER_PGTABLE_LEVELS &gt; 2 /* SWAPPER_TABLE_SHIFT是30, PTRS_PER_PTE是512 */ create_table_entry \\tbl, \\virt, SWAPPER_TABLE_SHIFT, PTRS_PER_PTE, \\tmp1, \\tmp2#endif .endm create_pgd_entry宏用来创建level0和和level1的页表. 虚拟地址的[47:39]在level0页表中进行索引, 索引到的entry指向level1的页表, 这里level1的页表就是level0页表的下一个PAGE. 对应的[38:30]在level1页表中进行索引, 索引的entry指向level2的页表. 也就是再下一个PAGE. 这里来分析下create_table_entry宏 12345678910111213141516171819.macro create_table_entry, tbl, virt, shift, ptrs, tmp1, tmp2/* 下面两条指令取出虚拟地址(virt)的[shift+9:shift], 作为index */lsr \\tmp1, \\virt, #\\shiftand \\tmp1, \\tmp1, #\\ptrs - 1 // table index/* * 下面两条指令计算出这一级页表对应virt的entry的值, 第一条指令计算entry指向的下一级 * 页表的物理地址, 第二条指令指定当前entry是PMD_TYPE_TABLE, 也就是表示当前entry * 指向的仍然是一个页目录, 具体看arm的architecture reference manual. */add \\tmp2, \\tbl, #PAGE_SIZEorr \\tmp2, \\tmp2, #PMD_TYPE_TABLE // address of next table and entry type/* * 使用之前计算的index来得到virt对应的entry的位置(tbl + index * 8byte), 然后把 * 页表entry存到那个位置 */str \\tmp2, [\\tbl, \\tmp1, lsl #3]/* tbl指向下一级页表, 方便下一次计算 */add \\tbl, \\tbl, #PAGE_SIZE // next level table page.endm level2的页表由create_block_map来配置. 1234567891011121314151617181920212223242526 .macro create_block_map, tbl, flags, phys, start, end /* SWAPPER_BLOCK_SHIFT是21, 把物理地址右移21bit, 剩下的就是entry中的地址 */ lsr \\phys, \\phys, #SWAPPER_BLOCK_SHIFT /* 这两条指令取出start的[29:21]作为level2页表的索引, 存在start中 */ lsr \\start, \\start, #SWAPPER_BLOCK_SHIFT and \\start, \\start, #PTRS_PER_PTE - 1 // table index /* * phys = flags | (phys &lt;&lt; 21), 很明显, 就是构建一条level2页表entry, * entry将虚拟地址start(前面两条指令计算之前的值)转换成物理地址phys(计算前的值) */ orr \\phys, \\flags, \\phys, lsl #SWAPPER_BLOCK_SHIFT // table entry /* 将end也计算成一个index, 方便后面的循环建立页表 */ lsr \\end, \\end, #SWAPPER_BLOCK_SHIFT and \\end, \\end, #PTRS_PER_PTE - 1 // table end index /* 将之前构建的页表entry存到level2页表对应的位置(由[29:21]索引) */9999: str \\phys, [\\tbl, \\start, lsl #3] // store the entry /* * 索引每次加1, 页表entry的物理地址每次加2M, 这样就能计算出下一条entry的内容 * 和存放路径了 */ add \\start, \\start, #1 // next entry add \\phys, \\phys, #SWAPPER_BLOCK_SIZE // next block /* 循环创建参数中start到end的地址的映射 */ cmp \\start, \\end b.ls 9999b .endm 综合上面的注释, 这里就是按照armv8 MMU的block条目来创建从start到end虚拟地址空间的映射, 每一个条目映射2M的地址空间. 这里把前面提到的x7保存的flags细说一下. 12345678910111213141516#define SWAPPER_MM_MMUFLAGS (PMD_ATTRINDX(MT_NORMAL) | SWAPPER_PMD_FLAGS)#define MT_NORMAL 4#define PMD_ATTRINDX(t) (_AT(pmdval_t, (t)) &lt;&lt; 2)#define SWAPPER_PMD_FLAGS (PMD_TYPE_SECT | PMD_SECT_AF | PMD_SECT_S)#define PMD_TYPE_SECT (_AT(pmdval_t, 1) &lt;&lt; 0)#define PMD_SECT_AF (_AT(pmdval_t, 1) &lt;&lt; 10)#define PMD_SECT_S (_AT(pmdval_t, 3) &lt;&lt; 8)typedef u64 pmdval_t;#ifdef __ASSEMBLY__#define _AC(X,Y) X#define _AT(T,X) X#else#define __AC(X,Y) (X##Y)#define _AC(X,Y) __AC(X,Y)#define _AT(T,X) ((T)(X))#endif 具体每个字段的含义可以查询arm手册来看, 大致来说, 这个flags表明, 这条entry指向的是一个2M的block, 这个block是一段普通的内存(不是device memory, 下篇内容我们还会继续说), 是已经访问过的, 是inner sharable的. 另外一些没有设定的位置为0也有一些含义, 如表示code是可执行的, 访问权限是EL0 RO, EL1 RW, 具体看arm的architecture reference manual. 123456789101112131415161718192021adrp x0, swapper_pg_dirmov_q x5, KIMAGE_VADDR + TEXT_OFFSET // compile time __va(_text)add x5, x5, x23 // add KASLR displacementcreate_pgd_entry x0, x5, x3, x6adrp x6, _end // runtime __pa(_end)adrp x3, _text // runtime __pa(_text)sub x6, x6, x3 // _end - _textadd x6, x6, x5 // runtime __va(_end)create_block_map x0, x7, x3, x5, x6/* * Since the page tables have been populated with non-cacheable * accesses (MMU disabled), invalidate the idmap and swapper page * tables again to remove any speculatively loaded cache lines. */adrp x0, idmap_pg_dirldr x1, =(IDMAP_DIR_SIZE + SWAPPER_DIR_SIZE + RESERVED_TTBR0_SIZE)dmb sybl __inval_dcache_arearet x28 后面的code参照前面的分析就很好理解了, 创建swapper的页表, 然后用dmb sy完成同步, 最后清空缓存. 这里由一个需要注意低地方, 跟之前说的KASLR有关. add x5, x5, x23在x5中保存_text的虚拟地址之后, 又加了x23, 这个x23就是之前保存的kaslr区域的大小. 也就是说把KEREL运行的虚拟地址进行了随机化.","tags":[{"name":"linux","slug":"linux","permalink":"http://gngshn.github.io/tags/linux/"},{"name":"arm64","slug":"arm64","permalink":"http://gngshn.github.io/tags/arm64/"}]},{"title":"arm64_linux启动流程分析04_KASLR","date":"2017-11-30T04:00:19.000Z","path":"2017/11/30/arm64-linux启动流程分析04-KASLR/","text":"这篇内容主要是说一下KASLR KASLR是让内核在一个随机的地址上运行的技术, 就是说内核的运行时地址是随机的, 每次启动会变化. 这样内核的符号对应的地址会变化并不是链接地址从而加强安全性. 这里画一个图 12345678910+---------------+-------+-------------+----------------------| | | || 2M align resv | KASLR | TEXT_OFFSET | KERNEL IMAGE| | | |+---------------+-------+-------------+----------------------| | || | +----内核在DDR中的位置.| +----------------- ARM64规定的KERNEL运行前必须放在 2M对齐地址+TEXT_OFFSET 的地方| 这里是经过KASLR技术relocate之后的位置.+----------------------------------------- DDR起始地址. 目前开启KASLR的内核启动有两种办法: bootloader在设备树中添加一个kaslr-seed的节点. 并赋予一个随机的偏移, 将KERNEL放在ARM64规定的2M对齐地址+TEXT_OFFSET的地方, 然后启动内核, 之后内核会将自己relocate到2M对齐地址+KASLR offset+TEXT_OFFSET的地方运行 bootloader把将KERNEL放在2M对齐地址+KASLR offset+TEXT_OFFSET的地方(KASLR offset小于2M), 然后启动内核, 之后内核会修复symbol, 然后在原地运行. 在上一篇文章中, 我们放了一段code没说 12adrp x23, __PHYS_OFFSETand x23, x23, MIN_KIMG_ALIGN - 1 // KASLR offset, defaults to 0 123#define __PHYS_OFFSET (KERNEL_START - TEXT_OFFSET)#define KERNEL_START _text#define MIN_KIMG_ALIGN SZ_2M 我们假设使用第二种启动办法, 那么__PHYS_OFFSET指向KASLR区域的最后面所在的物理地址. 第二条汇编就会计算出KASLR区域的大小然后保存在x23寄存器中. 对这个寄存器多加留意, 在调用start_kernel之前还会再用到. 对于KASLR的BOOT后面还需继续降到, 这里先放放, 我们先在下篇接着往下说.","tags":[{"name":"linux","slug":"linux","permalink":"http://gngshn.github.io/tags/linux/"},{"name":"arm64","slug":"arm64","permalink":"http://gngshn.github.io/tags/arm64/"}]},{"title":"arm64_linux启动流程分析03_设定当前core的启动状态","date":"2017-11-30T04:00:11.000Z","path":"2017/11/30/arm64-linux启动流程分析03-设定当前core的启动状态/","text":"接下来看el2_setup 123456789101112131415ENTRY(el2_setup) msr SPsel, #1 // We want to use SP_EL&#123;1,2&#125; mrs x0, CurrentEL cmp x0, #CurrentEL_EL2 b.eq 1f mrs x0, sctlr_el1CPU_BE( orr x0, x0, #(3 &lt;&lt; 24) ) // Set the EE and E0E bits for EL1CPU_LE( bic x0, x0, #(3 &lt;&lt; 24) ) // Clear the EE and E0E bits for EL1 msr sctlr_el1, x0 mov w0, #BOOT_CPU_MODE_EL1 // This cpu booted in EL1 isb ret1: mrs x0, sctlr_el2......省略...... 判断当前core的EL, 如果是EL1, 就设定使用little endian, CPU_BE和CPU_LE只有一条会编译进code. 之后返回BOOT_CPU_MODE_EL1, 如果EL2, 会初始化虚拟化相关的东西, 比较复杂, 先搁着. 12adrp x23, __PHYS_OFFSETand x23, x23, MIN_KIMG_ALIGN - 1 // KASLR offset, defaults to 0 这段code跟kaslr有关, 我们下篇文章来着重讲这个. 先接着往下看set_cpu_boot_mode_flag: 12345678910set_cpu_boot_mode_flag: adr_l x1, __boot_cpu_mode cmp w0, #BOOT_CPU_MODE_EL2 b.ne 1f add x1, x1, #41: str w0, [x1] // This CPU has booted in EL1 dmb sy dc ivac, x1 // Invalidate potentially stale cache line retENDPROC(set_cpu_boot_mode_flag) 在__boot_cpu_mode标签下保存了两个值 123ENTRY(__boot_cpu_mode) .long BOOT_CPU_MODE_EL2 .long BOOT_CPU_MODE_EL1 这段code的作用就是把保存的这两个值修改为一致, 且为当前CORE的启动EL, 其他CORE启动时也会运行这段code, 这样如果后面的CORE启动到了不同的EL就又会把这两个值修改为不一致, 从而可以判断出CPU的启动状态时不对的.","tags":[{"name":"linux","slug":"linux","permalink":"http://gngshn.github.io/tags/linux/"},{"name":"arm64","slug":"arm64","permalink":"http://gngshn.github.io/tags/arm64/"}]},{"title":"arm64_linux启动流程分析02_保存启动信息","date":"2017-11-30T04:00:01.000Z","path":"2017/11/30/arm64-linux启动流程分析02-保存启动信息/","text":"解上节, 我们先来看看bl preserve_boot_args12345678910111213preserve_boot_args: mov x21, x0 // x21=FDT adr_l x0, boot_args // record the contents of stp x21, x1, [x0] // x0 .. x3 at kernel entry stp x2, x3, [x0, #16] dmb sy // needed before dc ivac with // MMU off mov x1, #0x20 // 4 x 8 bytes b __inval_dcache_area // tail callENDPROC(preserve_boot_args) 代码的含义一目了然, 把存fdt内存地址的x0保存到x21寄存器. 然后把启动参数x0, x1, x2, x3全部保存到boot_args数组中. arm64 linux规定: Primary CPU general-purpose register settings x0 = physical address of device tree blob (dtb) in system RAM. x1 = 0 (reserved for future use) x2 = 0 (reserved for future use) x3 = 0 (reserved for future use) 这里值得注意的有几点 这里有用到adr_l, arm64并没有这个指令, 这是一个宏 1234567891011 .macro adr_l, dst, sym#ifndef MODULE adrp \\dst, \\sym add \\dst, \\dst, :lo12:\\sym#else movz \\dst, #:abs_g3:\\sym movk \\dst, #:abs_g2_nc:\\sym movk \\dst, #:abs_g1_nc:\\sym movk \\dst, #:abs_g0_nc:\\sym#endif .endm 可以看到, 这里的adr_l拆分成了两条指令, adrp + add, adrp指令最大寻址空间时+-4GB, 但是所寻址的地址是4KB对齐的. 所以这里在加了一个add指令来修正地址的低12bit, 从而实现了这个加载+-4GB任意位置的运行时地址的宏. __inval_dcache_area函数用来invalidate指定区域的dcache, 具体如下 12345678910111213141516171819202122232425262728ENTRY(__inval_dcache_area) /* FALLTHROUGH *//* * __dma_inv_area(start, size) * - start - virtual start address of region * - size - size in question */__dma_inv_area: add x1, x1, x0 dcache_line_size x2, x3 sub x3, x2, #1 tst x1, x3 // end cache line aligned? bic x1, x1, x3 b.eq 1f dc civac, x1 // clean &amp; invalidate D / U line1: tst x0, x3 // start cache line aligned? bic x0, x0, x3 b.eq 2f dc civac, x0 // clean &amp; invalidate D / U line b 3f2: dc ivac, x0 // invalidate D / U line3: add x0, x0, x2 cmp x0, x1 b.lo 2b dsb sy retENDPIPROC(__inval_dcache_area) 可以看到如果指定内存区域有跨越cacheline, 那么对两边跨越了cacheline的地址使用的clean + invalidate, 对于中间区域可以直接invalidate不用写回内存, 从而加快invalidate速度.","tags":[{"name":"linux","slug":"linux","permalink":"http://gngshn.github.io/tags/linux/"},{"name":"arm64","slug":"arm64","permalink":"http://gngshn.github.io/tags/arm64/"}]},{"title":"arm64_linux启动流程分析01_内核的入口","date":"2017-11-30T03:54:27.000Z","path":"2017/11/30/arm64-linux启动流程分析01-内核的入口/","text":"本次分析使用的linux内核的版本时 4.14.2, 读者可以自己下载了对照分析 首先看链接文件如下: 1234567891011ENTRY(_text)SECTIONS&#123; .....省略部分无关内容.... . = KIMAGE_VADDR + TEXT_OFFSET; .head.text : &#123; _text = .; HEAD_TEXT &#125; 可以看到入口点在_text. 而_text是指向.head.text段的起始位置. 所以内核是从.head.text开始运行的. 那么这个段是写什么内容呢? HEAD_TEXT是一个宏#define HEAD_TEXT *(.head.text) 在arch/arm64/kernel/head.S中有 1234567891011121314151617 __HEAD_head: /* * DO NOT MODIFY. Image header expected by Linux boot-loaders. */#ifdef CONFIG_EFI /* * This add instruction has no meaningful effect except that * its opcode forms the magic \"MZ\" signature required by UEFI. */ add x13, x18, #0x16 b stext#else b stext // branch to kernel start, magic .long 0 // reserved#endif ...省略后面的内容... 所以对应到code, 内核是从_head开始运行的. 这个开始点放置了一个arm64 linux的header: 12345678910u32 code0; /* Executable code */u32 code1; /* Executable code */u64 text_offset; /* Image load offset, little endian */u64 image_size; /* Effective Image size, little endian */u64 flags; /* kernel flags, little endian */u64 res2 = 0; /* reserved */u64 res3 = 0; /* reserved */u64 res4 = 0; /* reserved */u32 magic = 0x644d5241; /* Magic number, little endian, \"ARM\\x64\" */u32 res5; /* reserved (used for PE COFF offset) */ 这个头的前两个位置放置的是可执行code 在开启UEFI支持时, add x13, x18, #0x16这个code实际上是为了满足EFI格式的”MZ”头. 如果使用UEFI来启动kernel, 会识别出来并走UEFI启动的流程, 如果是普通的启动过程如使用uboot的booti进行引导, 那么第一条指令就是一条dummy指令. 第二条就跳转到stext运行了. 12345678910111213141516ENTRY(stext) bl preserve_boot_args bl el2_setup // Drop to EL1, w0=cpu_boot_mode adrp x23, __PHYS_OFFSET and x23, x23, MIN_KIMG_ALIGN - 1 // KASLR offset, defaults to 0 bl set_cpu_boot_mode_flag bl __create_page_tables /* * The following calls CPU setup code, see arch/arm64/mm/proc.S for * details. * On return, the CPU will be ready for the MMU to be turned on and * the TCR will have been set. */ bl __cpu_setup // initialise processor b __primary_switchENDPROC(stext) 这段代码就是内核启动是运行的初始化代码. 后面会分章节来详细描述.","tags":[{"name":"linux","slug":"linux","permalink":"http://gngshn.github.io/tags/linux/"},{"name":"arm64","slug":"arm64","permalink":"http://gngshn.github.io/tags/arm64/"}]},{"title":"libuv 源码分析(5) - 文件操作流程","date":"2017-09-01T04:02:01.000Z","path":"2017/09/01/libuv-源码分析-5-文件操作流程/","text":"by gngshn &#103;&#110;&#x67;&#115;&#x68;&#x6e;&#64;&#x67;&#x6d;&#97;&#x69;&#x6c;&#x2e;&#x63;&#x6f;&#109; 上一篇, 我们讲到了libuv的工作队列, 这一篇我们讲到的文件操作刚好就用到了工作队列. 刚好复习一下.先来看一段libuv文件操作的代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263#include &lt;assert.h&gt;#include &lt;stdio.h&gt;#include &lt;fcntl.h&gt;#include &lt;unistd.h&gt;#include &lt;uv.h&gt;void on_read(uv_fs_t *req);uv_fs_t open_req;uv_fs_t read_req;uv_fs_t write_req;static char buffer[1024];static uv_buf_t iov;void on_write(uv_fs_t *req) &#123; if (req-&gt;result &lt; 0) &#123; fprintf(stderr, \"Write error: %s\\n\", uv_strerror((int)req-&gt;result)); &#125; else &#123; uv_fs_read(uv_default_loop(), &amp;read_req, open_req.result, &amp;iov, 1, -1, on_read); &#125;&#125;void on_read(uv_fs_t *req) &#123; if (req-&gt;result &lt; 0) &#123; fprintf(stderr, \"Read error: %s\\n\", uv_strerror(req-&gt;result)); &#125; else if (req-&gt;result == 0) &#123; uv_fs_t close_req; // synchronous uv_fs_close(uv_default_loop(), &amp;close_req, open_req.result, NULL); &#125; else if (req-&gt;result &gt; 0) &#123; iov.len = req-&gt;result; uv_fs_write(uv_default_loop(), &amp;write_req, 1, &amp;iov, 1, -1, on_write); &#125;&#125;void on_open(uv_fs_t *req) &#123; // The request passed to the callback is the same as the one the call setup // function was passed. assert(req == &amp;open_req); if (req-&gt;result &gt;= 0) &#123; iov = uv_buf_init(buffer, sizeof(buffer)); uv_fs_read(uv_default_loop(), &amp;read_req, req-&gt;result, &amp;iov, 1, -1, on_read); &#125; else &#123; fprintf(stderr, \"error opening file: %s\\n\", uv_strerror((int)req-&gt;result)); &#125;&#125;int main(int argc, char **argv) &#123; uv_fs_open(uv_default_loop(), &amp;open_req, argv[1], O_RDONLY, 0, on_open); uv_run(uv_default_loop(), UV_RUN_DEFAULT); uv_fs_req_cleanup(&amp;open_req); uv_fs_req_cleanup(&amp;read_req); uv_fs_req_cleanup(&amp;write_req); return 0;&#125; 可以看到, 首先调用uv_fs_open, 然后在open的回调函数on_open中调用uv_fs_read读取文件, 之后在read的回调函数on_read中调用uv_fs_close(同步)或uv_fs_write(异步), 在write的回调函数中继续调用uv_fs_read从事时间将文件全部都出来然后写入到标准输出(1)中. 下面我们就一步一步跟踪libuv的api来看看libuv是如何处理文件操作的. uv_fs_openuv_fs_open的定义如下123456789101112int uv_fs_open(uv_loop_t* loop, uv_fs_t* req, const char* path, int flags, int mode, uv_fs_cb cb) &#123; INIT(OPEN); PATH; req-&gt;flags = flags; req-&gt;mode = mode; POST;&#125; 其中INIT, PATH, POST宏是为了减少重复代码, 后面将看见它在多个地方都有用到先来看看INIT宏:1234567891011121314#define INIT(subtype) \\ do &#123; \\ req-&gt;type = UV_FS; \\ if (cb != NULL) \\ uv__req_init(loop, req, UV_FS); \\ req-&gt;fs_type = UV_FS_ ## subtype; \\ req-&gt;result = 0; \\ req-&gt;ptr = NULL; \\ req-&gt;loop = loop; \\ req-&gt;path = NULL; \\ req-&gt;new_path = NULL; \\ req-&gt;cb = cb; \\ &#125; \\ while (0) 初始化req的属性, 值得注意的是, 当cb不为NULL(表示异步操作时), 需要调用uv__req_init把req注册到loop中.PATH宏主要是处理path变量是否需要复制一份, 因为如果在异步回调时, path是否在存在都不知道了.1234567891011121314#define PATH \\ do &#123; \\ assert(path != NULL); \\ if (cb == NULL) &#123; \\ req-&gt;path = path; \\ &#125; else &#123; \\ req-&gt;path = uv__strdup(path); \\ if (req-&gt;path == NULL) &#123; \\ uv__req_unregister(loop, req); \\ return -ENOMEM; \\ &#125; \\ &#125; \\ &#125; \\ while (0) POST宏也是根据同步和异步操作来进行不同的处理, 如果是同步操作, 直接调用uv__fs_work并返回结果. 如果是异步操作, 那么调用uv__work_submit将任务uv__fs_work交给工作队列(线程池)来做. 根据上一篇所讲, 工作队列完成工作后, 最终会调用uv__fs_done回调.123456789101112#define POST \\ do &#123; \\ if (cb != NULL) &#123; \\ uv__work_submit(loop, &amp;req-&gt;work_req, uv__fs_work, uv__fs_done); \\ return 0; \\ &#125; \\ else &#123; \\ uv__fs_work(&amp;req-&gt;work_req); \\ return req-&gt;result; \\ &#125; \\ &#125; \\ while (0) uv_fs_open就是上面的这些操作.我们再来看看uv_fs_read:12345678910111213141516171819202122232425262728int uv_fs_read(uv_loop_t* loop, uv_fs_t* req, uv_file file, const uv_buf_t bufs[], unsigned int nbufs, int64_t off, uv_fs_cb cb) &#123; if (bufs == NULL || nbufs == 0) return -EINVAL; INIT(READ); req-&gt;file = file; req-&gt;nbufs = nbufs; req-&gt;bufs = req-&gt;bufsml; if (nbufs &gt; ARRAY_SIZE(req-&gt;bufsml)) req-&gt;bufs = uv__malloc(nbufs * sizeof(*bufs)); if (req-&gt;bufs == NULL) &#123; if (cb != NULL) uv__req_unregister(loop, req); return -ENOMEM; &#125; memcpy(req-&gt;bufs, bufs, nbufs * sizeof(*bufs)); req-&gt;off = off; POST;&#125; 可以看到, 又一次用到了INIT和POST宏, 我们关注一下中间的部分. 这段代码就是调整一下bufs的大小并将bufs描述信息复制到req中. 如果调整失败就取消这个read请求. 后面的操作就跟open类似了, 根据是否有回调来决定用线程或同步的调用read相关的系统调用来处理以此read请求.对于write跟read类似, 我们就不说了除了用直接的request来处理文件的操作, 我们还可以用stream来处理, 接下来, 我们就来讲讲这部份. 先来直接看一段使用stream的代码:1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980#include &lt;stdio.h&gt;#include &lt;fcntl.h&gt;#include &lt;unistd.h&gt;#include &lt;string.h&gt;#include &lt;stdlib.h&gt;#include &lt;uv.h&gt;typedef struct &#123; uv_write_t req; uv_buf_t buf;&#125; write_req_t;uv_loop_t *loop;uv_pipe_t stdin_pipe;uv_pipe_t stdout_pipe;uv_pipe_t file_pipe;void alloc_buffer(uv_handle_t *handle, size_t suggested_size, uv_buf_t *buf) &#123; *buf = uv_buf_init((char*) malloc(suggested_size), suggested_size);&#125;void free_write_req(uv_write_t *req) &#123; write_req_t *wr = (write_req_t*) req; free(wr-&gt;buf.base); free(wr);&#125;void on_stdout_write(uv_write_t *req, int status) &#123; free_write_req(req);&#125;void on_file_write(uv_write_t *req, int status) &#123; free_write_req(req);&#125;void write_data(uv_stream_t *dest, size_t size, uv_buf_t buf, uv_write_cb cb) &#123; write_req_t *req = (write_req_t*) malloc(sizeof(write_req_t)); req-&gt;buf = uv_buf_init((char*) malloc(size), size); memcpy(req-&gt;buf.base, buf.base, size); uv_write((uv_write_t*) req, (uv_stream_t*)dest, &amp;req-&gt;buf, 1, cb);&#125;void read_stdin(uv_stream_t *stream, ssize_t nread, const uv_buf_t *buf) &#123; if (nread &lt; 0)&#123; if (nread == UV_EOF)&#123; // end of file uv_close((uv_handle_t *)&amp;stdin_pipe, NULL); uv_close((uv_handle_t *)&amp;stdout_pipe, NULL); uv_close((uv_handle_t *)&amp;file_pipe, NULL); &#125; &#125; else if (nread &gt; 0) &#123; write_data((uv_stream_t *)&amp;stdout_pipe, nread, *buf, on_stdout_write); write_data((uv_stream_t *)&amp;file_pipe, nread, *buf, on_file_write); &#125; // OK to free buffer as write_data copies it. if (buf-&gt;base) free(buf-&gt;base);&#125;int main(int argc, char **argv) &#123; loop = uv_default_loop(); uv_pipe_init(loop, &amp;stdin_pipe, 0); uv_pipe_open(&amp;stdin_pipe, 0); uv_pipe_init(loop, &amp;stdout_pipe, 0); uv_pipe_open(&amp;stdout_pipe, 1); uv_fs_t file_req; int fd = uv_fs_open(loop, &amp;file_req, argv[1], O_CREAT | O_RDWR, 0644, NULL); uv_pipe_init(loop, &amp;file_pipe, 0); uv_pipe_open(&amp;file_pipe, fd); uv_read_start((uv_stream_t*)&amp;stdin_pipe, alloc_buffer, read_stdin); uv_run(loop, UV_RUN_DEFAULT); return 0;&#125; 可以看到程序首先初始化三个uv pipe, 分别打开标准输入, 标准输出和参数表中的文件. 然后调用uv_read_start注册读事件, 会有两个回调函数: alloc_buffer(从而允许用户自己进行内存管理)和read_stdin(读完成后的回调). 然后调用uv_run启动event loop, 后面会在read_stdin中注册其他事件来完成输入的显示和保存.我们先来看看uv_pipe_init:12345678int uv_pipe_init(uv_loop_t* loop, uv_pipe_t* handle, int ipc) &#123; uv__stream_init(loop, (uv_stream_t*)handle, UV_NAMED_PIPE); handle-&gt;shutdown_req = NULL; handle-&gt;connect_req = NULL; handle-&gt;pipe_fname = NULL; handle-&gt;ipc = ipc; return 0;&#125; 123456789101112131415161718192021222324252627282930313233343536void uv__stream_init(uv_loop_t* loop, uv_stream_t* stream, uv_handle_type type) &#123; int err; uv__handle_init(loop, (uv_handle_t*)stream, type); stream-&gt;read_cb = NULL; stream-&gt;alloc_cb = NULL; stream-&gt;close_cb = NULL; stream-&gt;connection_cb = NULL; stream-&gt;connect_req = NULL; stream-&gt;shutdown_req = NULL; stream-&gt;accepted_fd = -1; stream-&gt;queued_fds = NULL; stream-&gt;delayed_error = 0; QUEUE_INIT(&amp;stream-&gt;write_queue); QUEUE_INIT(&amp;stream-&gt;write_completed_queue); stream-&gt;write_queue_size = 0; if (loop-&gt;emfile_fd == -1) &#123; err = uv__open_cloexec(\"/dev/null\", O_RDONLY); if (err &lt; 0) /* In the rare case that \"/dev/null\" isn't mounted open \"/\" * instead. */ err = uv__open_cloexec(\"/\", O_RDONLY); if (err &gt;= 0) loop-&gt;emfile_fd = err; &#125;#if defined(__APPLE__) stream-&gt;select = NULL;#endif /* defined(__APPLE_) */ uv__io_init(&amp;stream-&gt;io_watcher, uv__stream_io, -1);&#125; init部分还是比较简单的, 主要有uv__handle_init和uv__io_init. 前面说过了然后uv_pipe_open:1234567891011121314151617int uv_pipe_open(uv_pipe_t* handle, uv_file fd) &#123; int err; err = uv__nonblock(fd, 1); if (err) return err;#if defined(__APPLE__) err = uv__stream_try_select((uv_stream_t*) handle, &amp;fd); if (err) return err;#endif /* defined(__APPLE__) */ return uv__stream_open((uv_stream_t*)handle, fd, UV_STREAM_READABLE | UV_STREAM_WRITABLE);&#125; 123456789101112131415161718192021222324252627282930313233int uv__stream_open(uv_stream_t* stream, int fd, int flags) &#123;#if defined(__APPLE__) int enable;#endif if (!(stream-&gt;io_watcher.fd == -1 || stream-&gt;io_watcher.fd == fd)) return -EBUSY; assert(fd &gt;= 0); stream-&gt;flags |= flags; if (stream-&gt;type == UV_TCP) &#123; if ((stream-&gt;flags &amp; UV_TCP_NODELAY) &amp;&amp; uv__tcp_nodelay(fd, 1)) return -errno; /* TODO Use delay the user passed in. */ if ((stream-&gt;flags &amp; UV_TCP_KEEPALIVE) &amp;&amp; uv__tcp_keepalive(fd, 1, 60)) return -errno; &#125;#if defined(__APPLE__) enable = 1; if (setsockopt(fd, SOL_SOCKET, SO_OOBINLINE, &amp;enable, sizeof(enable)) &amp;&amp; errno != ENOTSOCK &amp;&amp; errno != EINVAL) &#123; return -errno; &#125;#endif stream-&gt;io_watcher.fd = fd; return 0;&#125; 非常简单, 除了对TCP进行特殊处理就是把fd保存到io_watcher中.123456789101112131415161718192021222324252627282930int uv_read_start(uv_stream_t* stream, uv_alloc_cb alloc_cb, uv_read_cb read_cb) &#123; assert(stream-&gt;type == UV_TCP || stream-&gt;type == UV_NAMED_PIPE || stream-&gt;type == UV_TTY); if (stream-&gt;flags &amp; UV_CLOSING) return -EINVAL; /* The UV_STREAM_READING flag is irrelevant of the state of the tcp - it just * expresses the desired state of the user. */ stream-&gt;flags |= UV_STREAM_READING; /* TODO: try to do the read inline? */ /* TODO: keep track of tcp state. If we've gotten a EOF then we should * not start the IO watcher. */ assert(uv__stream_fd(stream) &gt;= 0); assert(alloc_cb); stream-&gt;read_cb = read_cb; stream-&gt;alloc_cb = alloc_cb; uv__io_start(stream-&gt;loop, &amp;stream-&gt;io_watcher, POLLIN); uv__handle_start(stream); uv__stream_osx_interrupt_select(stream); return 0;&#125; 主要是uv__io_start和uv__handle_start, 前者把事件放到watcher_queue, 这里要注意event是先放在pevents中的, 后面poll的时候才放到events中, 后者启动handle(stream) 那么alloc_cb和read_cb是如何调用到的呢?在uv__pipe_init被调用时, 调用了uv__stream_init, 进而调用uv__io_init(&amp;stream-&gt;io_watcher, uv__stream_io, -1), 这里设定了poll事件的回调为io__stream_io:1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950static void uv__stream_io(uv_loop_t* loop, uv__io_t* w, unsigned int events) &#123; uv_stream_t* stream; stream = container_of(w, uv_stream_t, io_watcher); assert(stream-&gt;type == UV_TCP || stream-&gt;type == UV_NAMED_PIPE || stream-&gt;type == UV_TTY); assert(!(stream-&gt;flags &amp; UV_CLOSING)); if (stream-&gt;connect_req) &#123; uv__stream_connect(stream); return; &#125; assert(uv__stream_fd(stream) &gt;= 0); /* Ignore POLLHUP here. Even it it's set, there may still be data to read. */ if (events &amp; (POLLIN | POLLERR | POLLHUP)) uv__read(stream); if (uv__stream_fd(stream) == -1) return; /* read_cb closed stream. */ /* Short-circuit iff POLLHUP is set, the user is still interested in read * events and uv__read() reported a partial read but not EOF. If the EOF * flag is set, uv__read() called read_cb with err=UV_EOF and we don't * have to do anything. If the partial read flag is not set, we can't * report the EOF yet because there is still data to read. */ if ((events &amp; POLLHUP) &amp;&amp; (stream-&gt;flags &amp; UV_STREAM_READING) &amp;&amp; (stream-&gt;flags &amp; UV_STREAM_READ_PARTIAL) &amp;&amp; !(stream-&gt;flags &amp; UV_STREAM_READ_EOF)) &#123; uv_buf_t buf = &#123; NULL, 0 &#125;; uv__stream_eof(stream, &amp;buf); &#125; if (uv__stream_fd(stream) == -1) return; /* read_cb closed stream. */ if (events &amp; (POLLOUT | POLLERR | POLLHUP)) &#123; uv__write(stream); uv__write_callbacks(stream); /* Write queue drained. */ if (QUEUE_EMPTY(&amp;stream-&gt;write_queue)) uv__drain(stream); &#125;&#125; connect_req等到tcp再说. 这里会调用uv__read, 在这里面会调用alloc_cb分配内存, 对于ipc用uv__recvmsg读数据, 否则用read来读数据.可以看到stream根据不同的结果(如需要重读, 读出错, 读完成, 正常读)来用不同的方式处理并调用read_cb, 对于数据没有读到要求那么多是会置起UV_STREAM_READ_PARTIAL. write部分我就不分析了, 其实也比较类似. 后面就会进入libuv网络部分的分析了.","tags":[{"name":"libuv","slug":"libuv","permalink":"http://gngshn.github.io/tags/libuv/"}]},{"title":"libuv 源码分析(4) - libuv的工作队列(线程池)","date":"2017-09-01T03:59:03.000Z","path":"2017/09/01/libuv-源码分析-4-libuv的工作队列-线程池/","text":"by gngshn &#x67;&#x6e;&#x67;&#x73;&#x68;&#x6e;&#64;&#103;&#109;&#97;&#105;&#108;&#46;&#99;&#x6f;&#109; libuv通过uv_work_queue来交付任务给工作队列的, 这个api也是libuv实现文件异步操作的基础:1234UV_EXTERN int uv_queue_work(uv_loop_t* loop, uv_work_t* req, uv_work_cb work_cb, uv_after_work_cb after_work_cb); 这个工作队列实现方式就是把任务(work_cb)交给线程池来处理, 并且任务完成后, 调用相应的回调函数(after_work_cb).我们先来看看这个函数的实现:1234567891011121314int uv_queue_work(uv_loop_t* loop, uv_work_t* req, uv_work_cb work_cb, uv_after_work_cb after_work_cb) &#123; if (work_cb == NULL) return UV_EINVAL; uv__req_init(loop, req, UV_WORK); req-&gt;loop = loop; req-&gt;work_cb = work_cb; req-&gt;after_work_cb = after_work_cb; uv__work_submit(loop, &amp;req-&gt;work_req, uv__queue_work, uv__queue_done); return 0;&#125; 可以看到函数首先初始化这个请求(req), 然后就调用uv__work_submit完成剩余的工作.下面我们主要就来分析一下uv__work_submit的操作过程. 看看libuv是如何调用work_cb来完成任务并调用到after_work_cb回调函数的.12345678910void uv__work_submit(uv_loop_t* loop, struct uv__work* w, void (*work)(struct uv__work* w), void (*done)(struct uv__work* w, int status)) &#123; uv_once(&amp;once, init_once); w-&gt;loop = loop; w-&gt;work = work; w-&gt;done = done; post(&amp;w-&gt;wq);&#125; uv_once(&amp;once, init_once);用来初始化libuv的线程池, 只会被调用一次. 线程池中的线程都是执行这样一个函数:1234567891011121314151617static void worker(void* arg) &#123; struct uv__work* w; QUEUE* q; (void) arg; for (;;) &#123; uv_mutex_lock(&amp;mutex); while (QUEUE_EMPTY(&amp;wq)) &#123; idle_threads += 1; uv_cond_wait(&amp;cond, &amp;mutex); idle_threads -= 1; &#125; ...... &#125;&#125; 可以看到每个线程都是等待在uv_cond_wait(&amp;cond, &amp;mutex);.初始化完成后, 会调用post(&amp;w-&gt;wq):1234567static void post(QUEUE* q) &#123; uv_mutex_lock(&amp;mutex); QUEUE_INSERT_TAIL(&amp;wq, q); if (idle_threads &gt; 0) uv_cond_signal(&amp;cond); uv_mutex_unlock(&amp;mutex);&#125; 可以看到post(&amp;w-&gt;wq)就是把w挂到全局的wq上面, 然后调用uv_cond_signal, 这就会唤醒一个前面的正在等待的线程来处理这个任务, 一个线程唤醒后, 就会执行work函数的后续部分.1234567891011121314151617181920212223242526272829303132333435363738394041static void worker(void* arg) &#123; struct uv__work* w; QUEUE* q; (void) arg; for (;;) &#123; uv_mutex_lock(&amp;mutex); while (QUEUE_EMPTY(&amp;wq)) &#123; idle_threads += 1; uv_cond_wait(&amp;cond, &amp;mutex); idle_threads -= 1; &#125; q = QUEUE_HEAD(&amp;wq); if (q == &amp;exit_message) uv_cond_signal(&amp;cond); else &#123; QUEUE_REMOVE(q); QUEUE_INIT(q); /* Signal uv_cancel() that the work req is executing. */ &#125; uv_mutex_unlock(&amp;mutex); if (q == &amp;exit_message) break; w = QUEUE_DATA(q, struct uv__work, wq); w-&gt;work(w); uv_mutex_lock(&amp;w-&gt;loop-&gt;wq_mutex); w-&gt;work = NULL; /* Signal uv_cancel() that the work req is done executing. */ QUEUE_INSERT_TAIL(&amp;w-&gt;loop-&gt;wq, &amp;w-&gt;wq); uv_async_send(&amp;w-&gt;loop-&gt;wq_async); uv_mutex_unlock(&amp;w-&gt;loop-&gt;wq_mutex); &#125;&#125; 线程先取出一个链表的元素q.exit_message相关的操作是用来退出所有线程的, 在libuv退出时, post(&amp;exit_message);被调用, 这会让libuv的所有线程都退出.如果没有exit_message, 线程正常往后执行. 先把q从wq链表中删除, 然后调用w-&gt;work(就是最开始的work_cb, 相当于执行工作), 然后把w放到loop-&gt;wq中并调用uv_async_send向event loop发送信号. event loop会注意这个信号并作出相应的处理.为了弄清楚event loop是如何注意到这个信号的, 我们先来看看uv_async_send都干了什么12345678910int uv_async_send(uv_async_t* handle) &#123; /* Do a cheap read first. */ if (ACCESS_ONCE(int, handle-&gt;pending) != 0) return 0; if (cmpxchgi(&amp;handle-&gt;pending, 0, 1) == 0) uv__async_send(handle-&gt;loop); return 0;&#125; 这里async事件在还没有被处理时(penging=1)多次发送也只有一次生效判断handle-&gt;pending如果是1, 表示已经发送过并且还没处理, 所以直接返回.如果是0就表明没有pending事件, 原子的设置pending为1, 并调用uv__async_send, 这个函数会往loop-&gt;async_io_watcher.fd(一个eventfd)里面写入’\\n’, 从而event_loop会在epoll中发现. 发现后会调用相应的回调函数, 那么回调函数是什么呢?在uv_loop_init中会调用uv_async_init(loop, &amp;loop-&gt;wq_async, uv__work_done)来指定回调函数是uv__work_done:12345678910111213141516int uv_async_init(uv_loop_t* loop, uv_async_t* handle, uv_async_cb async_cb) &#123; int err; err = uv__async_start(loop); if (err) return err; uv__handle_init(loop, (uv_handle_t*)handle, UV_ASYNC); handle-&gt;async_cb = async_cb; handle-&gt;pending = 0; QUEUE_INSERT_TAIL(&amp;loop-&gt;async_handles, &amp;handle-&gt;queue); uv__handle_start(handle); return 0;&#125; uv__async_start中创建eventfd, 并将其POLLIN事件加入event loop, 事件发生时会调用uv__async_io12uv__io_init(&amp;loop-&gt;async_io_watcher, uv__async_io, pipefd[0]);uv__io_start(loop, &amp;loop-&gt;async_io_watcher, POLLIN); 然后把handle初始化并配置async_cb和pending加入到event loop的async_handles中, 并启动handle.事件发生后调用uv__async_io:1234567891011121314151617181920212223242526272829303132333435363738394041424344static void uv__async_io(uv_loop_t* loop, uv__io_t* w, unsigned int events) &#123; char buf[1024]; ssize_t r; QUEUE queue; QUEUE* q; uv_async_t* h; assert(w == &amp;loop-&gt;async_io_watcher); for (;;) &#123; r = read(w-&gt;fd, buf, sizeof(buf)); if (r == sizeof(buf)) continue; if (r != -1) break; if (errno == EAGAIN || errno == EWOULDBLOCK) break; if (errno == EINTR) continue; abort(); &#125; QUEUE_MOVE(&amp;loop-&gt;async_handles, &amp;queue); while (!QUEUE_EMPTY(&amp;queue)) &#123; q = QUEUE_HEAD(&amp;queue); h = QUEUE_DATA(q, uv_async_t, queue); QUEUE_REMOVE(q); QUEUE_INSERT_TAIL(&amp;loop-&gt;async_handles, q); if (cmpxchgi(&amp;h-&gt;pending, 1, 0) == 0) continue; if (h-&gt;async_cb == NULL) continue; h-&gt;async_cb(h); &#125;&#125; 可以看到这个函数先把eventfd的内容读空, 然后一次对async_handles中的元素判断其pending, 如果为1就原子的将其至0, 这也表示handle有待处理的异步通知, 因此就会调用h-&gt;async_cb(h)前面说过(uv_async_init(loop, &amp;loop-&gt;wq_async, uv__work_done))对于我们的线程池来说这个回调是uv__work_done:123456789101112131415161718192021void uv__work_done(uv_async_t* handle) &#123; struct uv__work* w; uv_loop_t* loop; QUEUE* q; QUEUE wq; int err; loop = container_of(handle, uv_loop_t, wq_async); uv_mutex_lock(&amp;loop-&gt;wq_mutex); QUEUE_MOVE(&amp;loop-&gt;wq, &amp;wq); uv_mutex_unlock(&amp;loop-&gt;wq_mutex); while (!QUEUE_EMPTY(&amp;wq)) &#123; q = QUEUE_HEAD(&amp;wq); QUEUE_REMOVE(q); w = container_of(q, struct uv__work, wq); err = (w-&gt;work == uv__cancelled) ? UV_ECANCELED : 0; w-&gt;done(w, err); &#125;&#125; 函数把loop-&gt;wq的元素挨个删除并调用done回调函数. 这个done回调函数就是我们前面说的after_work_cb回调.至此, libuv的工作队列的实现就说完了.","tags":[{"name":"libuv","slug":"libuv","permalink":"http://gngshn.github.io/tags/libuv/"}]},{"title":"libuv 源码阅读(3) - poll过程","date":"2017-09-01T03:54:55.000Z","path":"2017/09/01/libuv-源码分析-3-poll过程/","text":"by gngshn &#103;&#x6e;&#103;&#115;&#x68;&#x6e;&#x40;&#x67;&#109;&#x61;&#x69;&#108;&#x2e;&#x63;&#x6f;&#109; 上一篇, 我们将到libuv的event loop过程, 其中留了个悬念, 下面我们来解除这个悬念, 直接上函数, 这个函数特别长, 我们依旧直接在函数里面写注释把~123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263void uv__io_poll(uv_loop_t* loop, int timeout) &#123; /* A bug in kernels &lt; 2.6.37 makes timeouts larger than ~30 minutes * effectively infinite on 32 bits architectures. To avoid blocking * indefinitely, we cap the timeout and poll again if necessary. * * Note that \"30 minutes\" is a simplification because it depends on * the value of CONFIG_HZ. The magic constant assumes CONFIG_HZ=1200, * that being the largest value I have seen in the wild (and only once.) */ static const int max_safe_timeout = 1789569; static int no_epoll_pwait; static int no_epoll_wait; struct uv__epoll_event events[1024]; struct uv__epoll_event* pe; struct uv__epoll_event e; int real_timeout; QUEUE* q; uv__io_t* w; sigset_t sigset; uint64_t sigmask; uint64_t base; int have_signals; int nevents; int count; int nfds; int fd; int op; int i; /* 如果watcher_queue是空, 表明没有时间要等待, 可以直接返回 */ if (loop-&gt;nfds == 0) &#123; assert(QUEUE_EMPTY(&amp;loop-&gt;watcher_queue)); return; &#125; /* 以此在`watcher_queue`中取出事件 */ while (!QUEUE_EMPTY(&amp;loop-&gt;watcher_queue)) &#123; q = QUEUE_HEAD(&amp;loop-&gt;watcher_queue); QUEUE_REMOVE(q); QUEUE_INIT(q); w = QUEUE_DATA(q, uv__io_t, watcher_queue); assert(w-&gt;pevents != 0); assert(w-&gt;fd &gt;= 0); assert(w-&gt;fd &lt; (int) loop-&gt;nwatchers); /* 把需要监听的文件的事件加入/修改到epoll中 */ e.events = w-&gt;pevents; e.data = w-&gt;fd; if (w-&gt;events == 0) op = UV__EPOLL_CTL_ADD; else op = UV__EPOLL_CTL_MOD; /* XXX Future optimization: do EPOLL_CTL_MOD lazily if we stop watching * events, skip the syscall and squelch the events after epoll_wait(). */ if (uv__epoll_ctl(loop-&gt;backend_fd, op, w-&gt;fd, &amp;e)) &#123; if (errno != EEXIST) abort(); assert(op == UV__EPOLL_CTL_ADD); /* We've reactivated a file descriptor that's been watched before. */ if (uv__epoll_ctl(loop-&gt;backend_fd, UV__EPOLL_CTL_MOD, w-&gt;fd, &amp;e)) abort(); &#125; w-&gt;events = w-&gt;pevents; &#125; /* 对SIGPROF的屏蔽处理, 注意这里用到了两种方式, 取决于系统的支持程度epoll_pwait/pthread_sigmask */ sigmask = 0; if (loop-&gt;flags &amp; UV_LOOP_BLOCK_SIGPROF) &#123; sigemptyset(&amp;sigset); sigaddset(&amp;sigset, SIGPROF); sigmask |= 1 &lt;&lt; (SIGPROF - 1); &#125; assert(timeout &gt;= -1); base = loop-&gt;time; count = 48; /* Benchmarks suggest this gives the best throughput. */ real_timeout = timeout; for (;;) &#123; /* See the comment for max_safe_timeout for an explanation of why * this is necessary. Executive summary: kernel bug workaround. */ /* 见上面的英文注释 */ if (sizeof(int32_t) == sizeof(long) &amp;&amp; timeout &gt;= max_safe_timeout) timeout = max_safe_timeout; if (sigmask != 0 &amp;&amp; no_epoll_pwait != 0) if (pthread_sigmask(SIG_BLOCK, &amp;sigset, NULL)) abort(); /* 进行epoll调用, 等待事件 */ if (no_epoll_wait != 0 || (sigmask != 0 &amp;&amp; no_epoll_pwait == 0)) &#123; nfds = uv__epoll_pwait(loop-&gt;backend_fd, events, ARRAY_SIZE(events), timeout, sigmask); if (nfds == -1 &amp;&amp; errno == ENOSYS) no_epoll_pwait = 1; &#125; else &#123; nfds = uv__epoll_wait(loop-&gt;backend_fd, events, ARRAY_SIZE(events), timeout); if (nfds == -1 &amp;&amp; errno == ENOSYS) no_epoll_wait = 1; &#125; if (sigmask != 0 &amp;&amp; no_epoll_pwait != 0) if (pthread_sigmask(SIG_UNBLOCK, &amp;sigset, NULL)) abort(); /* Update loop-&gt;time unconditionally. It's tempting to skip the update when * timeout == 0 (i.e. non-blocking poll) but there is no guarantee that the * operating system didn't reschedule our process while in the syscall. */ /* 见上面英文注释 */ SAVE_ERRNO(uv__update_time(loop)); if (nfds == 0) &#123; assert(timeout != -1); if (timeout == 0) return; /* We may have been inside the system call for longer than |timeout| * milliseconds so we need to update the timestamp to avoid drift. */ goto update_timeout; &#125; if (nfds == -1) &#123; if (errno == ENOSYS) &#123; /* epoll_wait() or epoll_pwait() failed, try the other system call. */ assert(no_epoll_wait == 0 || no_epoll_pwait == 0); continue; &#125; if (errno != EINTR) abort(); if (timeout == -1) continue; if (timeout == 0) return; /* Interrupted by a signal. Update timeout and poll again. */ goto update_timeout; &#125; have_signals = 0; nevents = 0; /* 把所有poll到的事件描述信息放在数组的最后两个元素 */ assert(loop-&gt;watchers != NULL); loop-&gt;watchers[loop-&gt;nwatchers] = (void*) events; loop-&gt;watchers[loop-&gt;nwatchers + 1] = (void*) (uintptr_t) nfds; for (i = 0; i &lt; nfds; i++) &#123; pe = events + i; fd = pe-&gt;data; /* Skip invalidated events, see uv__platform_invalidate_fd */ if (fd == -1) continue; assert(fd &gt;= 0); assert((unsigned) fd &lt; loop-&gt;nwatchers); w = loop-&gt;watchers[fd]; if (w == NULL) &#123; /* File descriptor that we've stopped watching, disarm it. * * Ignore all errors because we may be racing with another thread * when the file descriptor is closed. */ uv__epoll_ctl(loop-&gt;backend_fd, UV__EPOLL_CTL_DEL, fd, pe); continue; &#125; /* Give users only events they're interested in. Prevents spurious * callbacks when previous callback invocation in this loop has stopped * the current watcher. Also, filters out events that users has not * requested us to watch. */ pe-&gt;events &amp;= w-&gt;pevents | POLLERR | POLLHUP; /* Work around an epoll quirk where it sometimes reports just the * EPOLLERR or EPOLLHUP event. In order to force the event loop to * move forward, we merge in the read/write events that the watcher * is interested in; uv__read() and uv__write() will then deal with * the error or hangup in the usual fashion. * * Note to self: happens when epoll reports EPOLLIN|EPOLLHUP, the user * reads the available data, calls uv_read_stop(), then sometime later * calls uv_read_start() again. By then, libuv has forgotten about the * hangup and the kernel won't report EPOLLIN again because there's * nothing left to read. If anything, libuv is to blame here. The * current hack is just a quick bandaid; to properly fix it, libuv * needs to remember the error/hangup event. We should get that for * free when we switch over to edge-triggered I/O. */ if (pe-&gt;events == POLLERR || pe-&gt;events == POLLHUP) pe-&gt;events |= w-&gt;pevents &amp; (POLLIN | POLLOUT | UV__POLLPRI); if (pe-&gt;events != 0) &#123; /* Run signal watchers last. This also affects child process watchers * because those are implemented in terms of signal watchers. */ /* 调用回调, 对于信号处理放在后面统一执行 */ if (w == &amp;loop-&gt;signal_io_watcher) have_signals = 1; else w-&gt;cb(loop, w, pe-&gt;events); nevents++; &#125; &#125; /* 执行信号处理程序 根据uv_loop_init看出, 这个程序是uv__signal_event, 由uv__io_init(&amp;loop-&gt;signal_io_watcher, uv__signal_event, loop-&gt;signal_pipefd[0]) 指定, 后面我们再来看这个函数 */ if (have_signals != 0) loop-&gt;signal_io_watcher.cb(loop, &amp;loop-&gt;signal_io_watcher, POLLIN); /* 删除两个描述符 */ loop-&gt;watchers[loop-&gt;nwatchers] = NULL; loop-&gt;watchers[loop-&gt;nwatchers + 1] = NULL; if (have_signals != 0) return; /* Event loop should cycle now so don't poll again. */ if (nevents != 0) &#123; if (nfds == ARRAY_SIZE(events) &amp;&amp; --count != 0) &#123; /* Poll for more events but don't block this time. */ timeout = 0; continue; &#125; return; &#125; if (timeout == 0) return; if (timeout == -1) continue;update_timeout: assert(timeout &gt; 0); real_timeout -= (loop-&gt;time - base); if (real_timeout &lt;= 0) return; timeout = real_timeout; &#125;&#125; 通过看libuv source code中的注释, 可以很快的理解这段代码到底做了写什么. 主要是加入待处理的事件到epoll, 然后poll, 发生事件就调用相关的回调函数. 另外, 为了规避BUG, 引入max_safe_timeout, 为了吞吐量, 引入poll的count.我们最后来看看信号处理函数干了写啥:1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374static void uv__signal_event(uv_loop_t* loop, uv__io_t* w, unsigned int events) &#123; uv__signal_msg_t* msg; uv_signal_t* handle; char buf[sizeof(uv__signal_msg_t) * 32]; size_t bytes, end, i; int r; bytes = 0; end = 0; do &#123; r = read(loop-&gt;signal_pipefd[0], buf + bytes, sizeof(buf) - bytes); if (r == -1 &amp;&amp; errno == EINTR) continue; if (r == -1 &amp;&amp; (errno == EAGAIN || errno == EWOULDBLOCK)) &#123; /* If there are bytes in the buffer already (which really is extremely * unlikely if possible at all) we can't exit the function here. We'll * spin until more bytes are read instead. */ if (bytes &gt; 0) continue; /* Otherwise, there was nothing there. */ return; &#125; /* Other errors really should never happen. */ if (r == -1) abort(); bytes += r; /* `end` is rounded down to a multiple of sizeof(uv__signal_msg_t). */ end = (bytes / sizeof(uv__signal_msg_t)) * sizeof(uv__signal_msg_t); for (i = 0; i &lt; end; i += sizeof(uv__signal_msg_t)) &#123; msg = (uv__signal_msg_t*) (buf + i); handle = msg-&gt;handle; if (msg-&gt;signum == handle-&gt;signum) &#123; assert(!(handle-&gt;flags &amp; UV_CLOSING)); handle-&gt;signal_cb(handle, handle-&gt;signum); &#125; handle-&gt;dispatched_signals++; if (handle-&gt;flags &amp; UV__SIGNAL_ONE_SHOT) uv__signal_stop(handle); /* If uv_close was called while there were caught signals that were not * yet dispatched, the uv__finish_close was deferred. Make close pending * now if this has happened. */ if ((handle-&gt;flags &amp; UV_CLOSING) &amp;&amp; (handle-&gt;caught_signals == handle-&gt;dispatched_signals)) &#123; uv__make_close_pending((uv_handle_t*) handle); &#125; &#125; bytes -= end; /* If there are any \"partial\" messages left, move them to the start of the * the buffer, and spin. This should not happen. */ if (bytes) &#123; memmove(buf, buf + end, bytes); continue; &#125; &#125; while (end == sizeof buf);&#125; 这个函数从之前讲过的的signal_pipefd[0]读取异常处理程序(uv__signal_handler)发送过来的消息(uv__signal_msg_t), 这个消息包括相应的handle的signum, 调用handle的回调函数, 并进行一些回收的处理. 具体细节可以追踪函数来看, 这里就不去细说了.到此我们就看完了libuv的event loop.","tags":[{"name":"libuv","slug":"libuv","permalink":"http://gngshn.github.io/tags/libuv/"}]},{"title":"libuv 源码分析(2) - event loop的运转","date":"2017-09-01T03:36:11.000Z","path":"2017/09/01/libuv-源码分析-2-event-loop的运转/","text":"by gngshn &#x67;&#110;&#x67;&#115;&#104;&#x6e;&#64;&#x67;&#x6d;&#97;&#x69;&#108;&#x2e;&#99;&#111;&#109; 上一篇文章我们讲到了 libuv的初始化, 现在乘热打铁, 我们接着看看 libuv的 event loop是如何运转的.对于 event loop的运转在其官方的文档中有详细的描述libuv design overviewevent loop的流程图如下下面我们就从源码的角度来看看整个过程.先看看一个最简单的 libuv程序:1234567891011int main() &#123; uv_loop_t *loop = malloc(sizeof(uv_loop_t)); uv_loop_init(loop); printf(\"Now quitting.\\n\"); uv_run(loop, UV_RUN_DEFAULT); uv_loop_close(loop); free(loop); return 0;&#125; 可以看到 libuv的 event loop过程在 uv_run中实现的. 我们下面来看看其过程.1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950int uv_run(uv_loop_t* loop, uv_run_mode mode) &#123; int timeout; int r; int ran_pending; r = uv__loop_alive(loop); if (!r) uv__update_time(loop); while (r != 0 &amp;&amp; loop-&gt;stop_flag == 0) &#123; uv__update_time(loop); uv__run_timers(loop); ran_pending = uv__run_pending(loop); uv__run_idle(loop); uv__run_prepare(loop); timeout = 0; if ((mode == UV_RUN_ONCE &amp;&amp; !ran_pending) || mode == UV_RUN_DEFAULT) timeout = uv_backend_timeout(loop); uv__io_poll(loop, timeout); uv__run_check(loop); uv__run_closing_handles(loop); if (mode == UV_RUN_ONCE) &#123; /* UV_RUN_ONCE implies forward progress: at least one callback must have * been invoked when it returns. uv__io_poll() can return without doing * I/O (meaning: no callbacks) when its timeout expires - which means we * have pending timers that satisfy the forward progress constraint. * * UV_RUN_NOWAIT makes no guarantees about progress so it's omitted from * the check. */ uv__update_time(loop); uv__run_timers(loop); &#125; r = uv__loop_alive(loop); if (mode == UV_RUN_ONCE || mode == UV_RUN_NOWAIT) break; &#125; /* The if statement lets gcc compile it to a conditional store. Avoids * dirtying a cache line. */ if (loop-&gt;stop_flag != 0) loop-&gt;stop_flag = 0; return r;&#125; 函数首先判断 event loop是否是 alive的, 如果不是, 更新 loop时间然后就退出了. 这里判断 alive的函数为:12345static int uv__loop_alive(const uv_loop_t* loop) &#123; return uv__has_active_handles(loop) || uv__has_active_reqs(loop) || loop-&gt;closing_handles != NULL;&#125; 可以看到, libuv判定 event loop存活(继续循环)的逻辑为有活动的 handle, request或者待关闭的 handle.当 uv__loop_alive返回 false时, 就不进行循环, 直接退出. 如果返回 true, 就进行循环.下面我们来分析一下这个循环的过程. uv__update_time(loop)细心的你们在阅读代码时, 可能会发现, libuv为了性能(效率)尽可能的再优化每一个细节, 比如 time更新这个地方12345678910111213141516171819202122232425262728293031uint64_t uv__hrtime(uv_clocktype_t type) &#123; static clock_t fast_clock_id = -1; struct timespec t; clock_t clock_id; /* Prefer CLOCK_MONOTONIC_COARSE if available but only when it has * millisecond granularity or better. CLOCK_MONOTONIC_COARSE is * serviced entirely from the vDSO, whereas CLOCK_MONOTONIC may * decide to make a costly system call. */ /* TODO(bnoordhuis) Use CLOCK_MONOTONIC_COARSE for UV_CLOCK_PRECISE * when it has microsecond granularity or better (unlikely). */ if (type == UV_CLOCK_FAST &amp;&amp; fast_clock_id == -1) &#123; if (clock_getres(CLOCK_MONOTONIC_COARSE, &amp;t) == 0 &amp;&amp; t.tv_nsec &lt;= 1 * 1000 * 1000) &#123; fast_clock_id = CLOCK_MONOTONIC_COARSE; &#125; else &#123; fast_clock_id = CLOCK_MONOTONIC; &#125; &#125; clock_id = CLOCK_MONOTONIC; if (type == UV_CLOCK_FAST) clock_id = fast_clock_id; if (clock_gettime(clock_id, &amp;t)) return 0; /* Not really possible. */ return t.tv_sec * (uint64_t) 1e9 + t.tv_nsec;&#125; CLOCK_MONOTONIC_COARSE这种类型的 clock精度足够就会用这种类型, 因为这种类型的 clock使用 vDSO, 可以降低系统调用的开销. uv__run_timers(loop)123456789101112131415161718void uv__run_timers(uv_loop_t* loop) &#123; struct heap_node* heap_node; uv_timer_t* handle; for (;;) &#123; heap_node = heap_min((struct heap*) &amp;loop-&gt;timer_heap); if (heap_node == NULL) break; handle = container_of(heap_node, uv_timer_t, heap_node); if (handle-&gt;timeout &gt; loop-&gt;time) break; uv_timer_stop(handle); uv_timer_again(handle); handle-&gt;timer_cb(handle); &#125;&#125; 所有的timer handles都是用 uv_timer_start调用来注册生效的. uv__run_timers就是把注册的 timers中到期的 timer去掉并注册回调, 具体过程为:具体流程为, 从 timer_heap(最小堆)取 timer, 这是最小的 timer, 如果 timer比现在的时间大, 表明没超时, 就可以退出了, 如果比现在的时间小, 表明已经超时, 就将 timer从 heap中删除, 然后调用 uv__handle_stop停止 handle. 再调用 uv_timer_again(会调用uv_timer_start)再次开启有 repeat值的 timer, 这次 timer的时间是按照 timer的 repeat时间来设定, 也就是 timer可以周期性触发. 最后调用 timer的 timer_cb回调.回到开启继续处理后面的 timer. 这里刚好遇到了 handle的操作, 之前没细说, 这里细说下:1234567891011121314151617181920212223242526272829303132333435#define uv__handle_start(h) \\ do &#123; \\ assert(((h)-&gt;flags &amp; UV__HANDLE_CLOSING) == 0); \\ if (((h)-&gt;flags &amp; UV__HANDLE_ACTIVE) != 0) break; \\ (h)-&gt;flags |= UV__HANDLE_ACTIVE; \\ if (((h)-&gt;flags &amp; UV__HANDLE_REF) != 0) uv__active_handle_add(h); \\ &#125; \\ while (0)#define uv__handle_stop(h) \\ do &#123; \\ assert(((h)-&gt;flags &amp; UV__HANDLE_CLOSING) == 0); \\ if (((h)-&gt;flags &amp; UV__HANDLE_ACTIVE) == 0) break; \\ (h)-&gt;flags &amp;= ~UV__HANDLE_ACTIVE; \\ if (((h)-&gt;flags &amp; UV__HANDLE_REF) != 0) uv__active_handle_rm(h); \\ &#125; \\ while (0)#define uv__handle_ref(h) \\ do &#123; \\ if (((h)-&gt;flags &amp; UV__HANDLE_REF) != 0) break; \\ (h)-&gt;flags |= UV__HANDLE_REF; \\ if (((h)-&gt;flags &amp; UV__HANDLE_CLOSING) != 0) break; \\ if (((h)-&gt;flags &amp; UV__HANDLE_ACTIVE) != 0) uv__active_handle_add(h); \\ &#125; \\ while (0)#define uv__handle_unref(h) \\ do &#123; \\ if (((h)-&gt;flags &amp; UV__HANDLE_REF) == 0) break; \\ (h)-&gt;flags &amp;= ~UV__HANDLE_REF; \\ if (((h)-&gt;flags &amp; UV__HANDLE_CLOSING) != 0) break; \\ if (((h)-&gt;flags &amp; UV__HANDLE_ACTIVE) != 0) uv__active_handle_rm(h); \\ &#125; \\ while (0) 上面这四个宏用来开启, 关闭, 引用, 解除引用handle, 这里要注意到几点: 在开启 handle时, 当 UV__HANDLE_ACTIVE已经置起来时(handle已经开启了), 就不进行后面的操作了, 所以可以重复调用 uv__handle_start不会产生问题. 关闭 handle时同理. 在引用 handle时, 当 UV__HANDLE_REF已经置起来时(handle已经引用过了), 就不再增加引用计数了, 所以重复调用, handle也只会增加一次引用计数, 解除引用 handle时同理. uv__active_handle_add和uv__active_handle_rm增加/减少的引用计数是loop的(而不是handle本身)引用计数, 字段为active_handles, 因此一个handle可以通过ref或unref来让一个handle是否影响到loop的active, 比如libuv的async_handle就通过uv__handle_unref来防止event loop无法退出. uv__run_pending1234567891011121314151617181920static int uv__run_pending(uv_loop_t* loop) &#123; QUEUE* q; QUEUE pq; uv__io_t* w; if (QUEUE_EMPTY(&amp;loop-&gt;pending_queue)) return 0; QUEUE_MOVE(&amp;loop-&gt;pending_queue, &amp;pq); while (!QUEUE_EMPTY(&amp;pq)) &#123; q = QUEUE_HEAD(&amp;pq); QUEUE_REMOVE(q); QUEUE_INIT(q); w = QUEUE_DATA(q, uv__io_t, pending_queue); w-&gt;cb(loop, w, POLLOUT); &#125; return 1;&#125; 很简单, 就是把pending_queue所有io事件取出来, 并调用相关回调. pending queue的加入等以后再说. uv__run_idle, uv__run_prepare以及后面的uv__run_check这三个调用分别操作idle_handles, prepare_handles以及check_handles. 这三种handle都是用下面的宏来操作:123456789101112131415161718192021222324252627282930313233int uv_##name##_start(uv_##name##_t* handle, uv_##name##_cb cb) &#123; \\ if (uv__is_active(handle)) return 0; \\ if (cb == NULL) return -EINVAL; \\ QUEUE_INSERT_HEAD(&amp;handle-&gt;loop-&gt;name##_handles, &amp;handle-&gt;queue); \\ handle-&gt;name##_cb = cb; \\ uv__handle_start(handle); \\ return 0; \\&#125; \\ \\int uv_##name##_stop(uv_##name##_t* handle) &#123; \\ if (!uv__is_active(handle)) return 0; \\ QUEUE_REMOVE(&amp;handle-&gt;queue); \\ uv__handle_stop(handle); \\ return 0; \\&#125; \\ \\void uv__run_##name(uv_loop_t* loop) &#123; \\ uv_##name##_t* h; \\ QUEUE queue; \\ QUEUE* q; \\ QUEUE_MOVE(&amp;loop-&gt;name##_handles, &amp;queue); \\ while (!QUEUE_EMPTY(&amp;queue)) &#123; \\ q = QUEUE_HEAD(&amp;queue); \\ h = QUEUE_DATA(q, uv_##name##_t, queue); \\ QUEUE_REMOVE(q); \\ QUEUE_INSERT_TAIL(&amp;loop-&gt;name##_handles, q); \\ h-&gt;name##_cb(h); \\ &#125; \\&#125; \\ \\void uv__##name##_close(uv_##name##_t* handle) &#123; \\ uv_##name##_stop(handle); \\&#125; 可以看出这三种操作很简单.start就是把 handle加入到相应的queue, 然后调用uv__handle_start, stop就是将handle移动出来然后调用uv__handle_stop, run就是就是依次执行queue上面所有handle的回调. uv_backend_timeout这个函数比较简单, 就是计算下一次poll的超时时间. 当然, 这个函数只在pending队列被执行前是空的(这样就保证123456789101112131415161718int uv_backend_timeout(const uv_loop_t* loop) &#123; if (loop-&gt;stop_flag != 0) return 0; if (!uv__has_active_handles(loop) &amp;&amp; !uv__has_active_reqs(loop)) return 0; if (!QUEUE_EMPTY(&amp;loop-&gt;idle_handles)) return 0; if (!QUEUE_EMPTY(&amp;loop-&gt;pending_queue)) return 0; if (loop-&gt;closing_handles) return 0; return uv__next_timeout(loop);&#125; 可以看到, 当 event loop确认自己还有事情要做的时候, 就会返回0, 表示下次poll是非阻塞的.uv__next_timeout比较简单, 就是从 timer_heap中得到最小超时的时间, 从而计算出下一次的超时时间, 如果 timer_heap是空, 表示下一次poll可以无限制的等待. uv__io_polluv__io_poll是整个 event loop中的最关键, 这部份内容较多, 我们下一篇来讲. uv__run_closing_handles这个函数用来关闭所有的待关闭的 handles, 通过调用 uv__finish_close来实现. 这个函数会关闭 handle并调用对应的回调函数, 这部份比较杂, 如果后面有机会再讲讲.12345678910111213static void uv__run_closing_handles(uv_loop_t* loop) &#123; uv_handle_t* p; uv_handle_t* q; p = loop-&gt;closing_handles; loop-&gt;closing_handles = NULL; while (p) &#123; q = p-&gt;next_closing; uv__finish_close(p); p = q; &#125;&#125; 整个 event loop后续部分就比较简单了, 就不再说了. 至此我们就看完了. 如果你把代码和官方的文档中的流程图对应一下, 就会发现是一致的.下一篇, 我们会来看看 libuv是如何进行 poll操作的.","tags":[{"name":"libuv","slug":"libuv","permalink":"http://gngshn.github.io/tags/libuv/"}]},{"title":"libuv 源码分析(1) - event loop的初始化","date":"2017-09-01T03:22:46.000Z","path":"2017/09/01/libuv-源码分析-1-event-loop的初始化/","text":"by gngshn &#103;&#x6e;&#103;&#x73;&#x68;&#110;&#64;&#103;&#x6d;&#97;&#x69;&#x6c;&#46;&#99;&#x6f;&#109; 从这里开始我将从linux的角度来看看libuv的工作原理, 如果您希望看到libuv跨平台的实现方式, 那你可能要失望了. 因为这一系列文章都将从linux的角度来写. 我们知道如果需要使用libuv的event loop需要通过uv_default_loop或者uv_loop_init来获得一个loop, 前者也是会调用后者的. 所以我们来看看uv_loop_init到底做了些什么, 我将在source code中进行直接写注释来描述相关字段的含义.123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112int uv_loop_init(uv_loop_t* loop) &#123; void* saved_data; int err; /* 创建一ipe, 往其中一端写能解锁读端, 这与信号处理有关 */ uv__signal_global_once_init(); saved_data = loop-&gt;data; memset(loop, 0, sizeof(*loop)); loop-&gt;data = saved_data; /* 保存libuv中timer的堆, 用来计算下次poll的超时时间 */ heap_init((struct heap*) &amp;loop-&gt;timer_heap); /* libuv线程池完成工作以后, 会把相关的request(uv_work_t)放在这个链表上, 从而libuv可以通过他来调用after_work_done回调 */ QUEUE_INIT(&amp;loop-&gt;wq); /* event loop中的所有的request */ QUEUE_INIT(&amp;loop-&gt;active_reqs); /* event loop中的idle handles, 这个handle每次loop都会调用一次 */ QUEUE_INIT(&amp;loop-&gt;idle_handles); /* event loop中的async_handles, 这个handle主要用来管理多线程的异步通知, 如libuv的工作队列的完成通知 */ QUEUE_INIT(&amp;loop-&gt;async_handles); /* event loop中的prepare handles, 这个handle每次loop都会调用一次 */ QUEUE_INIT(&amp;loop-&gt;check_handles); /* event loop中的check handles, 这个handle每次loop都会调用一次, 和prepare是一对, 分别在poll前后调用 */ QUEUE_INIT(&amp;loop-&gt;prepare_handles); /* event loop中所有的handle */ QUEUE_INIT(&amp;loop-&gt;handle_queue); /* event loop监听的描述符数量 */ loop-&gt;nfds = 0; /* event loop监听的事件的表(数组, 但不一定充满, 用fd来索引), 最后两个元素是特殊的 */ loop-&gt;watchers = NULL; /* watchers数组的大小 - 2 */ loop-&gt;nwatchers = 0; /* TODO */ QUEUE_INIT(&amp;loop-&gt;pending_queue); /* 所有待加入poll的事ueue, libuv通过它来确定需要往epoll中加入哪些描述符和事件 */ QUEUE_INIT(&amp;loop-&gt;watcher_queue); /* 需要关闭的handles链表 */ loop-&gt;closing_handles = NULL; /* 更新时间 */ uv__update_time(loop); /* 这里存放异步通知所用的eventfd或者pipe的描述符 */ loop-&gt;async_io_watcher.fd = -1; /* 和上面一样, 存放pipe对的另一个描述符, 如果是eventfd, 就是-1 */ loop-&gt;async_wfd = -1; /* uv信号处理回调所用的pipe, 信号处理函数往[1]写, event loop poll [0], 从而获取msg来处理信号 */ loop-&gt;signal_pipefd[0] = -1; loop-&gt;signal_pipefd[1] = -1; /* event loop的epoll描述符 */ loop-&gt;backend_fd = -1; /* TODO */ loop-&gt;emfile_fd = -1; /* timer id的counter, 为了给timer一个唯一的id, 每次创建timer, 这个值都加1 */ loop-&gt;timer_counter = 0; /* event loop的停止标志*/ loop-&gt;stop_flag = 0; /* 将backend_fd设为创建的epoll的描述符, 并初始化了inotify为-1 */ err = uv__platform_loop_init(loop); if (err) return err; /* 这里会初始化前面的signal_pipefd为一对pipe并将signal_pipefd[0]的POLLIN事件加入监听列表, 用来处理信号 */ /* 这里的child_watcher是用来处理子进程的退出的, 会在创建进程(uv_spam)时添加SIG_CHILD的处理 */ err = uv_signal_init(loop, &amp;loop-&gt;child_watcher); if (err) goto fail_signal_init; /* unref是为了不让event loop停不下来, 也就是event loop判定alive的条件去除这个handle, 后面文章会说 */ uv__handle_unref(&amp;loop-&gt;child_watcher); /* 标记为UV__HANDLE_INTERNAL, 从而handle不会被关闭, 也不会被uv_walk影响 */ loop-&gt;child_watcher.flags |= UV__HANDLE_INTERNAL; /* 所有子进程的队列 */ QUEUE_INIT(&amp;loop-&gt;process_handles); /* TODO */ err = uv_rwlock_init(&amp;loop-&gt;cloexec_lock); if (err) goto fail_rwlock_init; /* 用来保护loop-&gt;wq的保护锁 */ err = uv_mutex_init(&amp;loop-&gt;wq_mutex); if (err) goto fail_mutex_init; /* 创建eventfd放在async_io_watcher.fd(前面说过), 并把其POLLIN加入到监听事件中, 用来处理进程发来的异步通知 */ err = uv_async_init(loop, &amp;loop-&gt;wq_async, uv__work_done); if (err) goto fail_async_init; /* 前面讲过 */ uv__handle_unref(&amp;loop-&gt;wq_async); loop-&gt;wq_async.flags |= UV__HANDLE_INTERNAL; return 0;fail_async_init: uv_mutex_destroy(&amp;loop-&gt;wq_mutex);fail_mutex_init: uv_rwlock_destroy(&amp;loop-&gt;cloexec_lock);fail_rwlock_init: uv__signal_loop_cleanup(loop);fail_signal_init: uv__platform_loop_delete(loop); return err;&#125; 以上就是libuv的event loop的初始化流程, 后续我将分模块来讲解libuv各个功能模块的实现原理.","tags":[{"name":"libuv","slug":"libuv","permalink":"http://gngshn.github.io/tags/libuv/"}]},{"title":"正式开始书写自己的博客","date":"2017-04-15T12:47:14.000Z","path":"2017/04/15/正式开始书写自己的博客/","text":"之前一直不喜欢类似csdn一类的网站来写博客, 自己搭wordpress有太麻烦, 现在使用github page正式开启自己的博客之旅.这里主要会记录一些自己遇到的坑和一些学习的见解, 希望自己能坚持下去.","tags":[{"name":"随笔","slug":"随笔","permalink":"http://gngshn.github.io/tags/随笔/"}]},{"title":"mmap读写寄存器应注意不要越界","date":"2017-04-15T08:56:12.000Z","path":"2017/04/15/mmap读写寄存器应注意不要越界/","text":"前两天使用/dev/mem来修改寄存器时遇到了一个问题. 当时的情况是这样的:我有8K的寄存器空间需要访问, 但是当时我将mmap的空间大小写小了, 只写了1K, 相当于只映射了4K的空间. 但是当我操作到4K-8K的空间的时候, 读写看起来都没有问题, 读回来的数据和写入的数据是一致的. 但是实际寄存器上好像没有感受到一样(没有写入寄存器应有的现象), 刚开始一致怀疑是fpga的bitfile有问题. 后来无意发现了mmap指定的size不对, 改好了就可以了.那么这里有一个问题, 为什么访问4K-8K的空间没有发生segment fault呢?我做了一个实验, 下面有两个程序:program 0 - have mmap:123456789101112#include &lt;stdio.h&gt;#include &lt;unistd.h&gt;#include &lt;fcntl.h&gt;#include &lt;sys/mman.h&gt;int main(int argc, char **argv)&#123; int fd = open(\"a.txt\", O_RDWR | O_DSYNC); void *ptr = mmap(0, 1000, PROT_READ | PROT_WRITE, MAP_SHARED, fd, 0); printf(\"0x%016lx\\n\", (unsigned long)ptr); pause();&#125; program 1 - don’t have mmap:1234567#include &lt;stdio.h&gt;#include &lt;unistd.h&gt;int main(int argc, char **argv)&#123; pause();&#125; 先将程序运行起来, 然后通过/proc/[pid]/maps查看他们的线性地址分布情况:对于program 0:123456789101112131415161718192000400000-00401000 r-xp 00000000 fd:02 1051271 /home/gngshn/b00600000-00601000 r--p 00000000 fd:02 1051271 /home/gngshn/b00601000-00602000 rw-p 00001000 fd:02 1051271 /home/gngshn/b02293000-022b4000 rw-p 00000000 00:00 0 [heap]7f17bd677000-7f17bd834000 r-xp 00000000 fd:00 924939 /usr/lib64/libc-2.24.so7f17bd834000-7f17bda33000 ---p 001bd000 fd:00 924939 /usr/lib64/libc-2.24.so7f17bda33000-7f17bda37000 r--p 001bc000 fd:00 924939 /usr/lib64/libc-2.24.so7f17bda37000-7f17bda39000 rw-p 001c0000 fd:00 924939 /usr/lib64/libc-2.24.so7f17bda39000-7f17bda3d000 rw-p 00000000 00:00 07f17bda3d000-7f17bda62000 r-xp 00000000 fd:00 924461 /usr/lib64/ld-2.24.so7f17bdc3f000-7f17bdc41000 rw-p 00000000 00:00 07f17bdc5f000-7f17bdc60000 rw-s 00000000 fd:02 1066614 /home/gngshn/a.txt7f17bdc60000-7f17bdc62000 rw-p 00000000 00:00 07f17bdc62000-7f17bdc63000 r--p 00025000 fd:00 924461 /usr/lib64/ld-2.24.so7f17bdc63000-7f17bdc64000 rw-p 00026000 fd:00 924461 /usr/lib64/ld-2.24.so7f17bdc64000-7f17bdc65000 rw-p 00000000 00:00 07fff75117000-7fff75138000 rw-p 00000000 00:00 0 [stack]7fff75151000-7fff75153000 r--p 00000000 00:00 0 [vvar]7fff75153000-7fff75155000 r-xp 00000000 00:00 0 [vdso]ffffffffff600000-ffffffffff601000 r-xp 00000000 00:00 0 [vsyscall] 对于program 1:12345678910111213141516171800400000-00401000 r-xp 00000000 fd:02 1051230 /home/gngshn/a00600000-00601000 r--p 00000000 fd:02 1051230 /home/gngshn/a00601000-00602000 rw-p 00001000 fd:02 1051230 /home/gngshn/a7fb99a57a000-7fb99a737000 r-xp 00000000 fd:00 924939 /usr/lib64/libc-2.24.so7fb99a737000-7fb99a936000 ---p 001bd000 fd:00 924939 /usr/lib64/libc-2.24.so7fb99a936000-7fb99a93a000 r--p 001bc000 fd:00 924939 /usr/lib64/libc-2.24.so7fb99a93a000-7fb99a93c000 rw-p 001c0000 fd:00 924939 /usr/lib64/libc-2.24.so7fb99a93c000-7fb99a940000 rw-p 00000000 00:00 07fb99a940000-7fb99a965000 r-xp 00000000 fd:00 924461 /usr/lib64/ld-2.24.so7fb99ab42000-7fb99ab44000 rw-p 00000000 00:00 07fb99ab63000-7fb99ab65000 rw-p 00000000 00:00 07fb99ab65000-7fb99ab66000 r--p 00025000 fd:00 924461 /usr/lib64/ld-2.24.so7fb99ab66000-7fb99ab67000 rw-p 00026000 fd:00 924461 /usr/lib64/ld-2.24.so7fb99ab67000-7fb99ab68000 rw-p 00000000 00:00 07ffdbcd07000-7ffdbcd28000 rw-p 00000000 00:00 0 [stack]7ffdbcd31000-7ffdbcd33000 r--p 00000000 00:00 0 [vvar]7ffdbcd33000-7ffdbcd35000 r-xp 00000000 00:00 0 [vdso]ffffffffff600000-ffffffffff601000 r-xp 00000000 00:00 0 [vsyscall] 可以看到program 0比program 1多了两段地址空间 heap空间 a.txt的mmap空间heap空间是引入标准io时, 内部使用引入的; a.txt的mmap空间是程序里面调用mmap产生的.这里可以看见mmap访问后面的地址没有出错的原因了, 在mmap后面刚好有一段mmap空间7f17bdc60000-7f17bdc62000 rw-p 00000000 00:00 0, size为8K, 权限为读写, 这段空间是匿名映射空间, 当时我的读写都操作到这里了.因此在使用mmap操作的时候, 一定要注意操作内存不要过界, 程序很可能不会因为你的操作过界而发生segment fault, 但是却会和你期望的相差甚远.","tags":[{"name":"driver","slug":"driver","permalink":"http://gngshn.github.io/tags/driver/"},{"name":"linux","slug":"linux","permalink":"http://gngshn.github.io/tags/linux/"},{"name":"mmap","slug":"mmap","permalink":"http://gngshn.github.io/tags/mmap/"}]},{"title":"kernel中的IS_ENABLED","date":"2017-04-15T08:55:11.000Z","path":"2017/04/15/kernel中的IS-ENABLED/","text":"在kernel的代码中, 有时候会看见IS_ENABLED(CONFIG_XXXX)来测试某个Kconfig选项是否开启(即选中为y或者m). 如12if (IS_ENABLED(CONFIG_TIME_LOW_RES) &amp;&amp; timer-&gt;is_rel) rem -= hrtimer_resolution; 这里当TIME_LOW_RES这个Kconfig选项配置为y或m, 并且timer-&gt;is_rel不为0时调用rem -= hrtimer_resolution.那么这个是怎样实现的呢?首先在Kconfig中选中某个选项为y或m时, 在.config文件中就会由一个CONFIG_XXXXX=y或CONFIG_XXXXX=m, 并且会自动生成一个头文件autoconfig.h. 当选中为y时, 头文件中包含#define CONFIG_XXXXX 1, 当选中为m时, 头文件中包含#define CONFIG_XXXXX_MODULE 1, 当不选中是, 头文件中不包含相关语句.IS_ENABLED定义为1#define IS_ENABLED(option) __or(IS_BUILTIN(option), IS_MODULE(option)) IS_BUILTIN, IS_MODULE和__or分别定义为12#define IS_BUILTIN(option) __is_defined(option)#define IS_MODULE(option) __is_defined(option##_MODULE) 123#define __or(x, y) ___or(x, y)#define ___or(x, y) ____or(__ARG_PLACEHOLDER_##x, y)#define ____or(arg1_or_junk, y) __take_second_arg(arg1_or_junk 1, y) __is_defined定义为123#define __is_defined(x) ___is_defined(x)#define ___is_defined(val) ____is_defined(__ARG_PLACEHOLDER_##val)#define ____is_defined(arg1_or_junk) __take_second_arg(arg1_or_junk 1, 0) 在这之前定义了12#define __ARG_PLACEHOLDER_1 0,#define __take_second_arg(__ignored, val, ...) val 当#define CONFIG_XXXXX 1时__is_defined(1)展开为____is_defined(0,), 即__take_second_arg(0, 1, 0), 最终为1当CONFIG_XXXXX没有定义时__is_defined()展开为____is_defined()(因为没有定义ARG_PLACEHOLDER_), 即`take_second_arg(1, 0), 最终为0同样的方法可以理解__or. 因此IS_ENABLED主要是将没有定义的CONFIG_XXXXX`映射到0.","tags":[{"name":"driver","slug":"driver","permalink":"http://gngshn.github.io/tags/driver/"},{"name":"linux","slug":"linux","permalink":"http://gngshn.github.io/tags/linux/"}]},{"title":"使用docker来构建嵌入式开发环境","date":"2017-04-15T08:54:11.000Z","path":"2017/04/15/使用docker来构建嵌入式开发环境/","text":"我目前使用的一套嵌入式开发SDK是在ubuntu 14.04上构建的, 对于较新的ubuntu发行版或其他linux发行版是不能兼容的.比如目前我使用的是Fedora 25作为自己的系统, 那么要编译SDK会发生错误. 主要是libtools, autoconf等工具的兼容性问题. 为了解决这个问题可以使用docker来构建一套基于ubuntu14.04文件系统的编译系统, 从而实现在Fedora 25上编译SDK.首先贴上自己的Dockerfile123456789101112131415161718192021222324# rsdk builder docker# creater by gngshn# Jan 20 2017FROM ubuntu:14.04ENV TERM=xtermRUN echo &quot;dash dash/sh boolean false&quot; | debconf-set-selections \\ &amp;&amp; dpkg-reconfigure -p critical dash \\ &amp;&amp; apt-get update \\ &amp;&amp; apt-get install -y bc python dpkg lzma pkg-config libncurses5-dev \\ autoconf automake cmake libtool gettext texinfo gawk \\ &amp;&amp; dpkg --add-architecture i386 \\ &amp;&amp; apt-get update \\ &amp;&amp; apt-get install -y gcc-multilib lib32z1-dev lib32ncurses5-dev \\ &amp;&amp; apt-get clean \\ &amp;&amp; groupadd -g 1000 gngshn \\ &amp;&amp; useradd -u 1000 -g 1000 -G sudo -m gngshn \\ &amp;&amp; mkdir /home/gngshn/ipcam \\ &amp;&amp; chown gngshn:gngshn /home/gngshn/ipcam \\ &amp;&amp; echo &quot;root:xx&quot; | chpasswd \\ &amp;&amp; rm -rf /var/lib/apt/lists/* /tmp/* /var/tmp/*USER gngshnWORKDIR /home/gngshnENV TERM=xterm-256color \\ TERMINFO=/lib/terminfo 在Dockerfile中需要安装的包都是在ubuntu 14.04 docker中根据需要一步一步安装, 并最终确定需要的安装的所有的包.在上面的Dockerfile目录下执行sudo docker build -t ipcam_build ., docker就会从docker.io上下载ubuntu 14.04(也可以配置一些国内的docker加速器加速), 然后开始一层一层构建整个开发环境等ipcam_build构建完成后, 执行sudo docker run -it --name ipcam_build -v ~/ipcam:/home/gngshn/ipcam ipcam_build, 就可以进入到本开发环境的命令行了, 同时在docker 用户目录的ipcam下挂载了宿主机上的SDK(宿主机SDK的目录在~/ipcam).后续就可以在docker中直接编译SDK了.在docker命令行下按Ctrl+D就退出docker了, 当需要再次进入SDK编译环境时执行sudo docker start -ai ipcam_build就可以了.对于其他的嵌入式开发环境配置也是类似的. 这里就是一个简单的引导.","tags":[{"name":"docker","slug":"docker","permalink":"http://gngshn.github.io/tags/docker/"}]},{"title":"dev_mem的mmap分析","date":"2017-04-15T08:52:24.000Z","path":"2017/04/15/dev-mem的mmap分析/","text":"当用户空间需要实现驱动程序时, 需要操作底层的寄存器. 使用/dev/mem映射io内存空间就是一种常用的方式. /dev/mem 一般都是使用其 mmap 函数. 将底层的寄存器映射到用户空间地址. 下面对 /dev/mem的mmap函数进行一下简单的分析./dev/mem设备文件是由drivers/char/mem.c驱动创建. 该设备文件的file_operations为1234567891011static const struct file_operations __maybe_unused mem_fops = &#123; .llseek = memory_lseek, .read = read_mem, .write = write_mem, .mmap = mmap_mem, .open = open_mem,#ifndef CONFIG_MMU .get_unmapped_area = get_unmapped_area_mem, .mmap_capabilities = memory_mmap_capabilities,#endif&#125;; mmap_mem定义为:123456789101112131415161718192021222324252627282930313233static int mmap_mem(struct file *file, struct vm_area_struct *vma)&#123; size_t size = vma-&gt;vm_end - vma-&gt;vm_start; if (!valid_mmap_phys_addr_range(vma-&gt;vm_pgoff, size)) return -EINVAL; if (!private_mapping_ok(vma)) return -ENOSYS; if (!range_is_allowed(vma-&gt;vm_pgoff, size)) return -EPERM; if (!phys_mem_access_prot_allowed(file, vma-&gt;vm_pgoff, size, &amp;vma-&gt;vm_page_prot)) return -EINVAL; vma-&gt;vm_page_prot = phys_mem_access_prot(file, vma-&gt;vm_pgoff, size, vma-&gt;vm_page_prot); vma-&gt;vm_ops = &amp;mmap_mem_ops; /* Remap-pfn-range will mark the range VM_IO */ if (remap_pfn_range(vma, vma-&gt;vm_start, vma-&gt;vm_pgoff, size, vma-&gt;vm_page_prot)) &#123; return -EAGAIN; &#125; return 0;&#125; valid_mmap_phys_addr_range函数始终返回1.private_mapping_ok函数在由MMU的情况下始终返回1.range_is_allowed函数不配置CONFIG_STRICT_DEVMEM时始终返回1, 在配置CONFIG_STRICT_DEVMEM时, 齐定义如下:1234567891011121314static inline int range_is_allowed(unsigned long pfn, unsigned long size)&#123; u64 from = ((u64)pfn) &lt;&lt; PAGE_SHIFT; u64 to = from + size; u64 cursor = from; while (cursor &lt; to) &#123; if (!devmem_is_allowed(pfn)) return 0; cursor += PAGE_SIZE; pfn++; &#125; return 1;&#125; 在这个函数中, 会对需要映射的地址段进行检查, 在x86平台上, devmem_is_allowed定义为:12345678910int devmem_is_allowed(unsigned long pagenr)&#123; if (pagenr &lt; 256) return 1; if (iomem_is_exclusive(pagenr &lt;&lt; PAGE_SHIFT)) return 0; if (!page_is_ram(pagenr)) return 1; return 0;&#125; 在arm平台上, dev_mem_allowed定义为:12345678int devmem_is_allowed(unsigned long pfn)&#123; if (iomem_is_exclusive(pfn &lt;&lt; PAGE_SHIFT)) return 0; if (!page_is_ram(pfn)) return 1; return 0;j&#125; 在x86上, 前1M空间是预留给BIOS和一些其他X等应用使用的空间, 这段空间是允许映射的.iomem_is_exclusive定义为:12345678910111213141516171819202122232425262728293031323334353637383940414243/* * check if an address is reserved in the iomem resource tree * returns 1 if reserved, 0 if not reserved. */int iomem_is_exclusive(u64 addr)&#123; struct resource *p = &amp;iomem_resource; int err = 0; loff_t l; int size = PAGE_SIZE; if (!strict_iomem_checks) return 0; addr = addr &amp; PAGE_MASK; read_lock(&amp;resource_lock); for (p = p-&gt;child; p ; p = r_next(NULL, p, &amp;l)) &#123; /* * We can probably skip the resources without * IORESOURCE_IO attribute? */ if (p-&gt;start &gt;= addr + size) break; if (p-&gt;end &lt; addr) continue; /* * A resource is exclusive if IORESOURCE_EXCLUSIVE is set * or CONFIG_IO_STRICT_DEVMEM is enabled and the * resource is busy. */ if ((p-&gt;flags &amp; IORESOURCE_BUSY) == 0) continue; if (IS_ENABLED(CONFIG_IO_STRICT_DEVMEM) || p-&gt;flags &amp; IORESOURCE_EXCLUSIVE) &#123; err = 1; break; &#125; &#125; read_unlock(&amp;resource_lock); return err;&#125; 如果这段地址对应的resource标记为IORESOURCE_BUSY, 那么将映射失败.如果这段地址在ram中, 同样不允许映射.可以看出在不配置CONFIG_STRICT_DEVMEM时, /dev/mem的限制是最小的.phys_mem_access_prot_allowed函数返回1检查各种限制条件通过后, 接着调用phys_mem_access_prot(file, vma-&gt;vm_pgoff, size, vma-&gt;vm_page_prot);来改变被映射的内存区的的页面属性.对于arm64, 定义如下:12345678910pgprot_t phys_mem_access_prot(struct file *file, unsigned long pfn, unsigned long size, pgprot_t vma_prot)&#123; if (!pfn_valid(pfn)) return pgprot_noncached(vma_prot); else if (file-&gt;f_flags &amp; O_SYNC) return pgprot_writecombine(vma_prot); return vma_prot;&#125;EXPORT_SYMBOL(phys_mem_access_prot); 对于非内存空间, 使用nocached, 对于内存空间如果file打开了O_SYNC标志就使用write_combine.最后调用remap_pfn_range来分配页表, 讲用户空间的一段线性地址空间指向映射的区域.!","tags":[{"name":"mmap","slug":"mmap","permalink":"http://gngshn.github.io/tags/mmap/"}]}]